{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Jupyter notebooks do not have intellisense. If you like to enable it, add following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable intellisense\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary addition\n",
    "_What exactly will the RNN learn?_\n",
    "\n",
    "**RNN is going to learn the carry bit on its own!**\n",
    "\n",
    "\n",
    "| input1 | input2 | carry-in | sum | carry-out |\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "| 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 1 | 1 | 0 |\n",
    "| 0 | 1 | 0 | 1 | 0 |\n",
    "| 0 | 1 | 1 | 0 | 1 |\n",
    "| 1 | 0 | 0 | 1 | 0 |\n",
    "| 1 | 0 | 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 | 0 | 1 |\n",
    "| 1 | 1 | 1 | 1 | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples\n",
    "The first step, sample data is needed.\n",
    "One looup table is used to help us converting int to binary and vice versa\n",
    "\n",
    "int2binary (__lookup table__)\n",
    "\n",
    "| int | binary array |\n",
    "| :--- | :---: |\n",
    "| 0 | [0, 0, 0, 0, 0, 0, 0, 0] |\n",
    "| 1 | [0, 0, 0, 0, 0, 0, 0, 1] |\n",
    "| 2 | [0, 0, 0, 0, 0, 0, 1, 0] |\n",
    "...\n",
    "| 255 | [1, 1, 1, 1, 1, 1, 1, 1] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, numpy as np\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good practice to seed your random numbers. Your numbers will still be randomly distributed, but they'll be randomly distributed in exactly the same way each time you train. This makes it easier to see how your changes affect the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    def __init__(self, binary_dim):\n",
    "        # creating lookup table for converting int to binary\n",
    "        self.int2binary = {}\n",
    "        \n",
    "        self.largest_number = pow(2,binary_dim)\n",
    "        range_numbers = range(self.largest_number)\n",
    "        \n",
    "        # genrating corresponding binary array\n",
    "        # for example binary[0] = array([0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)\n",
    "        binary = np.unpackbits(np.array([range_numbers],dtype=np.uint8).T,axis=1)\n",
    "        \n",
    "        # adding binary array to int2binary (lookup table)\n",
    "        for i in range_numbers:\n",
    "            self.int2binary[i] = binary[i]\n",
    "    \n",
    "    # generate a sample addition problem (a + b = c)\n",
    "    def get_sample_addition_problem(self):\n",
    "        a_int = np.random.randint(self.largest_number/2) # int version # generate random int between [1,largest_number/2)\n",
    "        a = self.int2binary[a_int] # binary encoding\n",
    "\n",
    "        b_int = np.random.randint(self.largest_number/2) # int version\n",
    "        b = self.int2binary[b_int] # binary encoding\n",
    "\n",
    "        # true answer => summation\n",
    "        c_int = a_int + b_int\n",
    "        c = self.int2binary[c_int]\n",
    "\n",
    "        return a, b, c, a_int, b_int, c_int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activation(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, net):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(self, output):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sigmoid activation function**\n",
    "\n",
    "A sigmoid function maps any value to a value between 0 and 1. \n",
    "\n",
    "forward\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "backward\n",
    "$$ \\frac{\\partial \\sigma(x)}{\\partial x} =  \\sigma(x)(1- \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid_activation(activation):\n",
    "        \n",
    "    def forward(self, net):\n",
    "        return 1/(1 + np.exp(-net))\n",
    "    \n",
    "    def backward(self, output):\n",
    "        return output*(1 - output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In every layer, number of neurons along with activation function should be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network_layer(ABC):\n",
    "    \n",
    "    def __init__(self, neuron_count):\n",
    "        self.neuron_count = neuron_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class input_layer(network_layer): \n",
    "    \n",
    "    def forward(self, X, W_input):\n",
    "        return np.dot(X,W_input)\n",
    "    \n",
    "    def backward(self, a, b, time_step, W_hidden, binary_dim, hidden_layer_values):\n",
    "        \n",
    "        position = -time_step-1\n",
    "        \n",
    "        X = np.array([[a[position], b[position]]])\n",
    "        \n",
    "        if time_step == -binary_dim:            \n",
    "            x_0 = np.array([[a[0], b[0]]])\n",
    "            return x_0\n",
    "        \n",
    "        s_t = hidden_layer_values[time_step]\n",
    "        t1 = s_t*(1-s_t)\n",
    "        \n",
    "        backward_prev = self.backward(a,b,time_step-1, W_hidden, binary_dim, hidden_layer_values)\n",
    "        t2 = backward_prev * W_hidden\n",
    "        t3 = X + t2\n",
    "        return_value =  t1 * t3\n",
    "        \n",
    "        return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hiddenLayerUnfold:\n",
    "    \n",
    "    def __init__(self, neuron_count):\n",
    "        \n",
    "        # Save the values obtained at Hidden Layer of current state in a list to keep track\n",
    "        self.hidden_layer_values  = list()\n",
    "        \n",
    "        # Initially, there is no previous hidden state. So append \"0\" for that\n",
    "        self.hidden_layer_values.append(np.zeros(neuron_count))\n",
    "    \n",
    "    def save_previous_hidden_layer_value(self, previous_hidden_layer_value):\n",
    "        self.hidden_layer_values.append(copy.deepcopy(previous_hidden_layer_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hidden_layer(network_layer):\n",
    "    \n",
    "    def __init__(self, neuron_count):\n",
    "        super().__init__(neuron_count)\n",
    "        #self.hiddenLayerUnfold = hiddenLayerUnfold(neuron_count)\n",
    "        # Save the values obtained at Hidden Layer of current state in a list to keep track\n",
    "        self.hidden_layer_values  = list()\n",
    "        \n",
    "        # Initially, there is no previous hidden state. So append \"0\" for that\n",
    "        self.hidden_layer_values.append(np.zeros(neuron_count))\n",
    "    \n",
    "    def forward(self, input_layer_output, W_hidden):\n",
    "        prev_hidden = self.hidden_layer_values[-1]      \n",
    "        net_hidden = input_layer_output + np.dot(prev_hidden, W_hidden)\n",
    "        sigmoid = sigmoid_activation()\n",
    "        return sigmoid.forward(net_hidden)\n",
    "    \n",
    "    def backward(self, hidden_value_index, W_hidden, binary_dim):\n",
    "        if hidden_value_index == -binary_dim:\n",
    "            s_0 = self.hidden_layer_values[0]\n",
    "            return s_0\n",
    "        \n",
    "        s_t = self.hidden_layer_values[hidden_value_index]\n",
    "        t1 = s_t*(1-s_t)\n",
    "        s_t_1 = self.hidden_layer_values[hidden_value_index-1]\n",
    "        \n",
    "        backward_prev = self.backward(hidden_value_index-1, W_hidden, binary_dim)\n",
    "        t2 = backward_prev * W_hidden\n",
    "        t3 = s_t_1 + t2\n",
    "        return_value =  t1 * t3\n",
    "        \n",
    "        return return_value\n",
    "    \n",
    "    def save_previous_hidden_layer_value(self, previous_hidden_layer_value):\n",
    "        #self.hiddenLayerUnfold.save_previous_hidden_layer_value(previous_hidden_layer_value)\n",
    "        self.hidden_layer_values.append(copy.deepcopy(previous_hidden_layer_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class output_layer(network_layer):\n",
    "    \n",
    "    def forward(self, hidden_layer_output, W_output):\n",
    "        net_output = np.dot(hidden_layer_output, W_output)\n",
    "        sigmoid = sigmoid_activation()\n",
    "        return sigmoid.forward(net_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weight:\n",
    "    \n",
    "    @staticmethod\n",
    "    def GetWeightMatrix(first_dimension, second_dimension):\n",
    "        return 2*np.random.random((first_dimension,second_dimension)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean squared error function**\n",
    "\n",
    "https://en.wikipedia.org/wiki/Mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_function():\n",
    "    \n",
    "    @staticmethod\n",
    "    def mse(target_value, predicted_value):\n",
    "        return np.mean((target_value - predicted_value)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class utility:\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_result(overallError, a_int, b_int, c, d):    \n",
    "        print(\"Error:\" + str(overallError))\n",
    "        print(\"Pred:\" + str(d))\n",
    "        print(\"True:\" + str(c))\n",
    "        out = 0\n",
    "        for index, x in enumerate(reversed(d)):\n",
    "            out += x * pow(2, index)\n",
    "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "The general algorithm is\n",
    "\n",
    "   1. First, present the input pattern and propagate it through the network to get the output.\n",
    "    \n",
    "   2. Then compare the predicted output to the expected output and calculate the error.\n",
    "\n",
    "   3. Then calculate the derivates of the error with respect to the network weights\n",
    "    \n",
    "   4. Try to adjust the weights so that the error is minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "\n",
    "This is our process to predict summation of two bits in each step:\n",
    "\n",
    "**input layer**\n",
    "\n",
    "X = two input bits (a,b)\n",
    "\n",
    "$ net_{input} = X \\times  W_{input} $\n",
    "\n",
    "No activation function is used in this layer\n",
    "\n",
    "**hidden layer**\n",
    "\n",
    "$ net_{hidden} = net_{input} + prev_{hidden} \\times W_{hidden} $\n",
    "\n",
    "activation function, which is used in this layer, is sigmoid\n",
    "\n",
    "$ A_{hidden} = A(net_{hidden}) = \\sigma(net_{hidden}) $\n",
    "\n",
    "**output layer**\n",
    "\n",
    "$ net_{output} = A_{hidden} \\times  W_{output} $\n",
    "\n",
    "$ \\hat{y}\\ (predited\\ value) = A_{output} = A(net_{output}) = \\sigma(net_{output}) $\n",
    "\n",
    "predited_value = one bit used for the output of the RNN (a+b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation throw time (BPTT)\n",
    "\n",
    "BPTT works by unrolling all input timesteps. Each timestep has one input time step, one output time step and one copy of the network. Then the errors are calculated and accumulated for each timestep. The network is then rolled back to update the weights.\n",
    "\n",
    "As can be seen in forward phase, three weights, following weights, are used to compute predicted value, therefore, these weights should be updated for next iteration.\n",
    "\n",
    "$ W\\_output $\n",
    "\n",
    "$ W\\_hidden $\n",
    "\n",
    "$ W\\_input $\n",
    "\n",
    "### Chain rule\n",
    "\n",
    "By applying BPTT, they can be updated. In this algorithm, **chain rule** is utilized.\n",
    "\n",
    "mean squared error is used for cost function:\n",
    "\n",
    "$ E = \\frac{1}{2}(y - \\hat{y})^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$ W_{output} $**\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial W_{output}} = \\frac{\\partial E}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial net_{output}} \\times \\frac{\\partial net_{output}}{\\partial w_{output}} $$\n",
    "\n",
    "To compute equation above, following computation are needed:\n",
    "\n",
    "$ \\frac{\\partial E}{\\partial \\hat{y}} = \\frac{\\partial \\frac{1}{2}(y - \\hat{y})^2 }{\\partial \\hat{y}} = 2 \\times \\frac{1}{2} \\times -1 \\times (y - \\hat{y}) = -(y - \\hat{y}) $\n",
    "\n",
    "$ \\frac{\\partial \\hat{y}}{\\partial net_{output}} = \\frac{\\partial \\ \\partial (net_{output})} {\\partial net_{output}} =  \\sigma (net_{output}) (1-\\sigma (net_{output})) = \\hat{y}(1-\\hat{y})$\n",
    "\n",
    "$ \\frac{\\partial net_{output}}{\\partial w_{output}} =  \\frac{\\partial A_{hidden} \\times  W_{output}}{\\partial w_{output}} = A_{hidden}$\n",
    "\n",
    "By putting these results into main equation, we end up with following equation:\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial W_{output}} = -(y - \\hat{y}) \\times \\hat{y}(1-\\hat{y}) \\times A_{hidden} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that unlike the corresponding equation in the previous example, the\n",
    "equations for $ W_{hidden}$ and $W_{input}$ are **recursive**.\n",
    "\n",
    "The derivative at the current time depends on the derivative at the previous time.\n",
    "\n",
    "**$ W_{hidden} $**\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial W_{hidden}} = \\frac{\\partial E}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial net_{output}} \\times \\frac{\\partial net_{output}}{\\partial A_{hidden}} \\times \\frac{\\partial A_{hidden}}{\\partial net_{hidden}} \\times \\frac{\\partial net_{hidden}}{\\partial W_{hidden}} $$\n",
    "\n",
    "To compute equation above, following computation are needed:\n",
    "\n",
    "$ \\frac{\\partial E}{\\partial \\hat{y}} = -(y - \\hat{y}) $\n",
    "\n",
    "$ \\frac{\\partial \\hat{y}}{\\partial net_{output}} = \\hat{y}(1-\\hat{y})$\n",
    "\n",
    "$ \\frac{\\partial net_{output}}{\\partial A_{hidden}} =  \\frac{\\partial A_{hidden} \\times  W_{output}}{\\partial  A_{hidden}} = w_{output}$\n",
    "\n",
    "$ \\frac{\\partial A_{hidden}}{\\partial net_{hidden}} = \\frac{\\partial \\sigma (net_{hidden})} {\\partial net_{hidden}} =  \\sigma (net_{hidden}) (1-\\sigma (net_{hidden})) = A_{hidden}(1-A_{hidden})$\n",
    "\n",
    "$ \\frac{\\partial net_{hidden}}{\\partial w_{hidden}} =  \\frac{\\partial \\ (net_{input} + prev_{hidden} \\times W_{hidden})}{\\partial w_{hidden}} = prev_{hidden} + \\frac{\\partial prev_{hidden}}{\\partial w_{hidden}}\\times w_{hidden}$\n",
    "\n",
    "By putting these results into the main equation, we end up with following equation:\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial W_{hidden}} = -(y - \\hat{y}) \\times \\hat{y}(1-\\hat{y}) \\times w_{output} \\times A_{hidden}(1-A_{hidden}) \\times (prev_{hidden} + \\frac{\\partial prev_{hidden}}{\\partial w_{hidden}}\\times w_{hidden})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$ W_{input} $**\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial W_{input}} = \\frac{\\partial E}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial net_{output}} \\times \\frac{\\partial net_{output}}{\\partial A_{hidden}} \\times \\frac{\\partial A_{hidden}}{\\partial net_{hidden}} \\times \\frac{\\partial net_{hidden}}{\\partial net_{input}} \\times \\frac{\\partial net_{input}}{\\partial W_{input}}$$\n",
    "\n",
    "To compute equation above, following computation are needed:\n",
    "\n",
    "$ \\frac{\\partial E}{\\partial \\hat{y}} = -(y - \\hat{y}) $\n",
    "\n",
    "$ \\frac{\\partial \\hat{y}}{\\partial net_{output}} = \\hat{y}(1-\\hat{y})$\n",
    "\n",
    "$ \\frac{\\partial net_{output}}{\\partial A_{hidden}} = w_{output}$\n",
    "\n",
    "$ \\frac{\\partial A_{hidden}}{\\partial W_{input}} = \\frac{\\partial A_{hidden}}{\\partial net_{hidden}} \\times \\frac{\\partial net_{hidden}}{\\partial W_{input}} $\n",
    "\n",
    "$ \\frac{\\partial A_{hidden}}{\\partial net_{hidden}} = A_{hidden}(1-A_{hidden}) $\n",
    "\n",
    "$ \\frac{\\partial net_{hidden}}{\\partial W_{input}} =  \\frac{\\partial \\ (net_{input} + prev_{hidden} \\times W_{hidden})}{\\partial W_{input}} =  \\frac{\\partial \\ (X \\times  W_{input} + prev_{hidden} \\times W_{hidden})}{\\partial W_{input}} = X +  \\frac{\\partial \\ ( prev_{hidden} \\times W_{hidden})}{\\partial W_{input}} = X + \\frac{\\partial \\ prev_{hidden}}{\\partial W_{input} }\\times W_{hidden} $\n",
    "\n",
    "$ \\frac{\\partial A_{hidden}}{\\partial W_{input}} = A_{hidden}(1-A_{hidden}) \\times (X + \\frac{\\partial \\ prev_{hidden}}{\\partial W_{input} }\\times W_{hidden}) $\n",
    "\n",
    "By putting these results into main equation, we end up with following equation:\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial W_{input}} = -(y - \\hat{y}) \\times \\hat{y}(1-\\hat{y}) \\times w_{output} \\times A_{hidden}(1-A_{hidden}) \\times (X + \\frac{\\partial prev_{hidden}}{\\partial W_{input} }\\times W_{hidden}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_binary_addition_rnn:\n",
    "    \n",
    "    def __init__(self, binary_dim, hidden_dimension, learning_rate):\n",
    "        \n",
    "        self.binary_dim = binary_dim\n",
    "        input_dimension = 2\n",
    "        output_dimension = 1    \n",
    "        \n",
    "        # predicated_values\n",
    "        self.predicated_values = np.zeros(self.binary_dim)        \n",
    "        \n",
    "        # layers\n",
    "        self.input_layer = input_layer(input_dimension)\n",
    "        self.hidden_layer = hidden_layer(hidden_dimension)\n",
    "        self.output_layer = output_layer(output_dimension)\n",
    "        \n",
    "        # initialize weights\n",
    "        self.W_input = weight.GetWeightMatrix(input_dimension, hidden_dimension)\n",
    "        self.W_hidden = weight.GetWeightMatrix(hidden_dimension, hidden_dimension)\n",
    "        self.W_output = weight.GetWeightMatrix(hidden_dimension, output_dimension)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.overallError = 0\n",
    "        \n",
    "    def feed_forward(self, a, b, c):\n",
    "        \n",
    "         # Array to save predicted outputs (binary encoded)\n",
    "        d = np.zeros_like(c)\n",
    "    \n",
    "        # position: location of the bit amongst binary_dim-1 bits; for example, starting point \"0\"; \"0 - 7\"\n",
    "        for position in range(binary_dim):\n",
    "\n",
    "            location = binary_dim - position - 1\n",
    "            X = np.array([[a[location], b[location]]])           \n",
    "            \n",
    "            # ----------- forward ---------------\n",
    "            # input_layer forward\n",
    "            input_layer_output = self.input_layer.forward(X, self.W_input)            \n",
    "            \n",
    "            # hidden_layer forward\n",
    "            hidden_layer_output = self.hidden_layer.forward(input_layer_output, self.W_hidden)\n",
    "            \n",
    "            # Save the hidden layer to be used in BPTT            \n",
    "            self.hidden_layer.save_previous_hidden_layer_value(hidden_layer_output)\n",
    "            \n",
    "            # self.output_layer.forward\n",
    "            # predicated_value is a \"guess\" for each input matrix. \n",
    "            # We can now compare how well it did by subtracting the true answer (y) from the guess (predicated_value). \n",
    "            # output_error is just a vector of positive and negative numbers reflecting how much the network missed.\n",
    "            predicated_value = self.output_layer.forward(hidden_layer_output, self.W_output)          \n",
    "\n",
    "            # Round off the values to nearest \"0\" or \"1\" and save it to a list\n",
    "            d[location] = np.round(predicated_value[0][0])   \n",
    "            \n",
    "        return d, self.predicated_values\n",
    "\n",
    "    def back_propagate(self, a, b, c, predicated_values):\n",
    "        \n",
    "        # Initialize Updated Weights Values\n",
    "        W_output_update = np.zeros_like(self.W_output)\n",
    "        W_hidden_update = np.zeros_like(self.W_hidden)\n",
    "        W_input_update = np.zeros_like(self.W_input)        \n",
    "        \n",
    "        # for position in range(self.binary_dim-1, -1, -1):  # binary_dim=8=> position: 7->0\n",
    "        for position in range(self.binary_dim):           \n",
    "\n",
    "            y = np.array([[c[position]]]).T        \n",
    "            \n",
    "            # sigmoid\n",
    "            sigmoid = sigmoid_activation()\n",
    "          \n",
    "            hidden_value_index = -position-1\n",
    "            A_hidden = self.hidden_layer.hidden_layer_values[hidden_value_index]\n",
    "          \n",
    "            # update W_output ----------------------------------------------------         \n",
    "            y_hat = predicated_values[position]            \n",
    "            dy_hat = (y-y_hat)\n",
    "            \n",
    "            # W_output---------------------\n",
    "            dnet_output = dy_hat * sigmoid.backward(y_hat)\n",
    "            dw_output = dnet_output* A_hidden.T           \n",
    "            W_output_update += dw_output     \n",
    "\n",
    "            # W_hidden ---------------------\n",
    "            dA_hidden = dnet_output*self.W_output            \n",
    "                    \n",
    "            t3 = self.hidden_layer.backward(hidden_value_index, self.W_hidden, self.binary_dim)\n",
    "            t4 = dA_hidden*t3            \n",
    "            W_hidden_update += t4        \n",
    "            \n",
    "            # W_input ---------------------\n",
    "            t_in_3 = self.input_layer.backward(a, b, hidden_value_index, self.W_hidden, self.binary_dim, self.hidden_layer.hidden_layer_values)\n",
    "            t_in_4 = dA_hidden*t_in_3            \n",
    "            W_input_update += t_in_4\n",
    "            \n",
    "            \n",
    "        self.W_output += W_output_update * self.learning_rate\n",
    "        self.W_hidden += W_hidden_update * self.learning_rate\n",
    "        self.W_input += W_input_update * self.learning_rate\n",
    "   \n",
    "    \n",
    "    def train(self, epochs_count):\n",
    "        \n",
    "        data = dataset(self.binary_dim)        \n",
    "        \n",
    "        # This for loop \"iterates\" multiple times over the training code to optimize our network to the dataset.\n",
    "        for epoch in range(epochs_count):\n",
    "            \n",
    "            overallError = 0\n",
    "            \n",
    "            # sample a + b = c\n",
    "            # for example: 2 + 3 = 5 => (a) 00000010 + (b) 00000011 = (c) 00000101\n",
    "            a, b, c, a_int, b_int, c_int = data.get_sample_addition_problem()\n",
    "            \n",
    "            # where we'll store our best guess (binary encoded)\n",
    "            # desired predictions => d\n",
    "            d = np.zeros_like(c)  \n",
    "            \n",
    "            d, predicated_values = self.feed_forward(a, b, c)\n",
    "            \n",
    "            self.back_propagate(a, b, c, predicated_values)\n",
    "    \n",
    "            # Print out the Progress of the RNN\n",
    "            if (epoch % 1000 == 0):\n",
    "                utility.print_result(overallError, a_int, b_int, c, d)\n",
    "                \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dim = 8\n",
    "hidden_dimension = 16\n",
    "learning_rate = 0.1\n",
    "rnn = simple_binary_addition_rnn(binary_dim, hidden_dimension, learning_rate)\n",
    "rnn.train(10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
