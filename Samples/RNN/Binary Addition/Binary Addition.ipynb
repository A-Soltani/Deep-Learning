{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Jupyter notebooks do not have intellisense. If you like to enable it, add following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable intellisense\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary addition\n",
    "_What exactly will the RNN learn ?_\n",
    "\n",
    "**RNN is going to learn the carry bit on its own!**\n",
    "\n",
    "\n",
    "| input1 | input2 | carry-in | sum | carry-out |\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "| 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 1 | 1 | 0 |\n",
    "| 0 | 1 | 0 | 1 | 0 |\n",
    "| 0 | 1 | 1 | 0 | 1 |\n",
    "| 1 | 0 | 0 | 1 | 0 |\n",
    "| 1 | 0 | 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 | 0 | 1 |\n",
    "| 1 | 1 | 1 | 1 | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples\n",
    "The first step, sample data is needed.\n",
    "One looup table is used to help us converting int to binary and vice versa\n",
    "\n",
    "int2binary (__lookup table__)\n",
    "\n",
    "| int | binary array |\n",
    "| :--- | :---: |\n",
    "| 0 | [0, 0, 0, 0, 0, 0, 0, 0] |\n",
    "| 1 | [0, 0, 0, 0, 0, 0, 0, 1] |\n",
    "| 2 | [0, 0, 0, 0, 0, 0, 1, 0] |\n",
    "...\n",
    "| 255 | [1, 1, 1, 1, 1, 1, 1, 1] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, numpy as np\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    def __init__(self, binary_dim):\n",
    "        # creating lookup table for converting int to binary\n",
    "        self.int2binary = {}\n",
    "        \n",
    "        self.largest_number = pow(2,binary_dim)\n",
    "        range_numbers = range(self.largest_number)\n",
    "        \n",
    "        # genrating corresponding binary array\n",
    "        # for example binary[0] = array([0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)\n",
    "        binary = np.unpackbits(np.array([range_numbers],dtype=np.uint8).T,axis=1)\n",
    "        \n",
    "        # adding binary array to int2binary (lookup table)\n",
    "        for i in range_numbers:\n",
    "            self.int2binary[i] = binary[i]\n",
    "    \n",
    "    # generate a sample addition problem (a + b = c)\n",
    "    @staticmethod\n",
    "    def get_sample_addition_problem(self):\n",
    "        a_int = np.random.randint(self.largest_number/2) # int version # generate random int between [1,largest_number/2)\n",
    "        a = self.int2binary[a_int] # binary encoding\n",
    "\n",
    "        b_int = np.random.randint(self.largest_number/2) # int version\n",
    "        b = self.int2binary[b_int] # binary encoding\n",
    "\n",
    "        # true answer => summation\n",
    "        c_int = a_int + b_int\n",
    "        c = self.int2binary[c_int]\n",
    "\n",
    "        return a, b, c, a_int, b_int, c_int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activation(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(net):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(output):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sigmoid activation function**\n",
    "\n",
    "forward\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "backward\n",
    "$$ \\frac{\\partial \\sigma(x)}{\\partial x} =  \\sigma(x)(1- \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid_activation(activation):\n",
    "        \n",
    "    def forward(net):\n",
    "        return 1/(1 + np.exp(-net))\n",
    "    \n",
    "    def backward(output):\n",
    "        return output*(1 - output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In every layer, number of neurons along with activation function should be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network_layer(ABC):\n",
    "    def __init__(self, neuron_count, activation_function = None):\n",
    "        self.neuron_count = neuron_count\n",
    "        self.activation_function = activation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class input_layer(network_layer): \n",
    "    def forward(X, W_input):\n",
    "        return np.dot(X,W_input)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weight:\n",
    "    \n",
    "    @staticmethod\n",
    "    def GetWeightMatrix(first_dimension, second_dimension):\n",
    "        return 2*np.random.random((first_dimension,second_dimension)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_function(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def compute(target_value, predicted_value):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean squared error function**\n",
    "\n",
    "https://en.wikipedia.org/wiki/Mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mse_loss_function(loss_function):\n",
    "    \n",
    "    def compute(target_value, predicted_value):\n",
    "        return np.mean((target_value - predicted_value)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class utility:\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_result(overallError, a_int, b_int, c, d):    \n",
    "        print(\"Error:\" + str(overallError))\n",
    "        print(\"Pred:\" + str(d))\n",
    "        print(\"True:\" + str(c))\n",
    "        out = 0\n",
    "        for index, x in enumerate(reversed(d)):\n",
    "            out += x * pow(2, index)\n",
    "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_binary_addition_rnn:\n",
    "    \n",
    "    def __init__(self,binary_dim, input_dimension, output_dimension, hidden_dimension, learning_rate):\n",
    "        \n",
    "        self.binary_dim = binary_dim\n",
    "        \n",
    "        # initialize weights\n",
    "        self.W_input = weight.GetWeightMatrix(input_dimension, hidden_dimension)\n",
    "        self.W_hidden = weight.GetWeightMatrix(hidden_dimension, hidden_dimension)\n",
    "        self.W_output = weight.GetWeightMatrix(hidden_dimension, output_dimension)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.overallError = 0\n",
    "        \n",
    "        # Save the values obtained at Hidden Layer of current state in a list to keep track\n",
    "        self.hidden_layer_values  = list()\n",
    "        \n",
    "        # Initially, there is no previous hidden state. So append \"0\" for that\n",
    "        self.hidden_layer_values.append(np.zeros(hidden_dimension))\n",
    "        \n",
    "        # update values for weights\n",
    "        self.output_layer_deltas = list()\n",
    "        \n",
    "    def feed_forward(a, b, c):   \n",
    "    \n",
    "        # position: location of the bit amongst binary_dim-1 bits; for example, starting point \"0\"; \"0 - 7\"\n",
    "        for position in range(binary_dim):\n",
    "\n",
    "            # With increasing value of position, the bit location of \"a\" and \"b\" decreases from \"7 -> 0\"\n",
    "            # and each iteration computes the sum of corresponding bit of \"a\" and \"b\".\n",
    "            # ex. for position = 0, X = [a[7],b[7]], 7th bit of a and b.\n",
    "\n",
    "            location = binary_dim - position - 1\n",
    "            X = np.array([[a[location], b[location]]])\n",
    "\n",
    "            # Actual value for (a+b) = c, c is an array of 8 bits, so take transpose to compare bit by bit with X value.        \n",
    "            target = np.array([[c[location]]]).T\n",
    "\n",
    "            # Values computed at current hidden layer\n",
    "            # [dot product of Input(X) and Weights(W_in)] + [dot product of previous hidden layer values and Weights (W_h)]\n",
    "            # W_hidden: weight from previous step hidden layer to current step hidden layer\n",
    "            # W_in: weights from current step input to current hidden layer\n",
    "            # at\n",
    "\n",
    "            # in https://github.com/peterroelants/peterroelants.github.io/blob/master/notebooks/RNN_implementation/rnn-implementation-part01.ipynb\n",
    "            # update_state(xk, sk, wx, wRec)\n",
    "            net_1 = np.dot(X,W_in) + np.dot(hidden_layer_values[-1],W_h) \n",
    "\n",
    "            # ht\n",
    "            layer_1_output = sigmoid(net_1)\n",
    "\n",
    "            # Save the hidden layer to be used later\n",
    "            # Recurrent\n",
    "            \"\"\"\n",
    "            forward_states(X, wx, wRec= w_h)\n",
    "            Unfold the network and compute all state activations \n",
    "            given the input X, input weights (wx), and recursive weights \n",
    "            (wRec). Return the state activations in a matrix, the last \n",
    "            column S[:,-1] contains the final activations.\n",
    "            \"\"\"\n",
    "            hidden_layer_values.append(copy.deepcopy(layer_1_output)) \n",
    "\n",
    "            # The new output using new Hidden layer values\n",
    "            # ot\n",
    "            net_2 = np.dot(layer_1_output, W_out)\n",
    "\n",
    "            # y_pred = sigma(ot)\n",
    "            layer_2_output = sigmoid(net_2)        \n",
    "\n",
    "            # Delta rule\n",
    "            # Calculate the error\n",
    "            # Cost function\n",
    "            # C = 1/2 x (y_true - y_pred)^2\n",
    "            output_error = target - layer_2_output\n",
    "\n",
    "            # Save the error deltas at each step as it will be propagated back\n",
    "            output_layer_deltas.append((output_error)*sigmoid_output_to_derivative(layer_2_output))\n",
    "\n",
    "            # Save the sum of error at each binary position\n",
    "            overallError += np.abs(output_error[0])\n",
    "\n",
    "            # Round off the values to nearest \"0\" or \"1\" and save it to a list\n",
    "            d[location] = np.round(layer_2_output[0][0])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __train__(self, epochs_count):\n",
    "        \n",
    "        for epoch in range(epochs_count):\n",
    "            \n",
    "            # sample a + b = c\n",
    "            # for example: 2 + 3 = 5 => (a) 00000010 + (b) 00000011 = (c) 00000101\n",
    "            a, b, c, a_int, b_int, c_int = dataset.get_sample_addition_problem()\n",
    "            \n",
    "            # where we'll store our best guess (binary encoded)\n",
    "            # desired predictions => d\n",
    "            d = np.zeros_like(c)  \n",
    "            \n",
    "            feed_forward(a, b ,c)\n",
    "            \n",
    "            back_propagating(a, b)\n",
    "    \n",
    "            # Print out the Progress of the RNN\n",
    "            if (epoch % 1000 == 0):\n",
    "                print_result(overallError, a_int, b_int, c, d)\n",
    "                \n",
    "    \n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
