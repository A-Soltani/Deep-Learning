{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary addition\n",
    "_What exactly will the RNN learn?_\n",
    "\n",
    "**RNN is going to learn the carry bit on its own!**\n",
    "\n",
    "\n",
    "| input1 | input2 | carry-in | sum | carry-out |\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "| 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 1 | 1 | 0 |\n",
    "| 0 | 1 | 0 | 1 | 0 |\n",
    "| 0 | 1 | 1 | 0 | 1 |\n",
    "| 1 | 0 | 0 | 1 | 0 |\n",
    "| 1 | 0 | 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 | 0 | 1 |\n",
    "| 1 | 1 | 1 | 1 | 1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# importing \"collections\" for deque operations \n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples\n",
    "To train and test our RNN, data set is needed.\n",
    "Samples in the dataset include a, b and c. Following samples show how they are shown:\n",
    "\n",
    "| a | b | c | \n",
    "| :---: | :---: | :---:|\n",
    "| [0 0 0 1 0 0 1 0] | [0 1 0 0 1 1 0 1] | [0 1 0 1 1 1 1 1]\n",
    "| [0 0 0 0 1 1 1 0] | [0 0 0 1 1 0 0 1] | [0 0 1 0 0 1 1 1]\n",
    "| [0 1 0 1 0 0 1 1] | [0 0 1 0 1 1 1 1] | [1 0 0 0 0 0 1 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good practice to seed your random numbers. Your numbers will still be randomly distributed, but they'll be randomly distributed in exactly the same way each time you train. This makes it easier to see how your changes affect the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_utility:\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_data(samples_count, binary_dim):\n",
    "        \n",
    "        largest_number = pow(2,binary_dim)\n",
    "        \n",
    "        samples = list()\n",
    "        for i in range(samples_count):\n",
    "            \n",
    "            a = np.random.randint(largest_number/2)\n",
    "            b = np.random.randint(largest_number/2)\n",
    "            # true answer => summation\n",
    "            c = a + b            \n",
    "                    \n",
    "            int_array = np.array([[a], [b], [c]], dtype=np.uint8)\n",
    "            \n",
    "            binary_array = np.unpackbits(int_array, axis=1)\n",
    "            \n",
    "            samples.append(binary_array)\n",
    "            \n",
    "        return samples\n",
    "\n",
    "    @staticmethod\n",
    "    def get_inputs_and_target(sample):\n",
    "         a = sample[0]\n",
    "         b = sample[1]\n",
    "         c = sample[2]\n",
    "         return a, b, c  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class utility:\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_result(a, b, c, predicated_values, epoch):  \n",
    "        d = np.zeros_like(c)\n",
    "        for position in range(len(predicated_values)):             \n",
    "             d[position] = np.round(predicated_values[position][0][0])\n",
    "        \n",
    "        print(\"epoch:\", epoch)\n",
    "        print(\"a:   \" + str(a))\n",
    "        print(\"b:   \" + str(b))        \n",
    "        print(\"----------------------\")\n",
    "        print(\"c:   \" + str(c))\n",
    "        print(\"Pred:\" + str(d))\n",
    "        print(\"============================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture\n",
    "Following architecture is used to addition of two bits in each step:\n",
    "\n",
    "<img src=\"./images/network_architecture.jpg\"><img>\n",
    "<center>Figure 1</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation model\n",
    "\n",
    "Following model is used to compute addition of two bits in each step:\n",
    "\n",
    "<img src=\"./images/forward_one_step.jpg\"><img>\n",
    "<center>Figure 2</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN gates\n",
    "\n",
    "As can be seen in comutation model, there are two types of gates in our RNN:\n",
    "    1. multiply gate\n",
    "    2. add gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiply_gate:\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(inputs, weights):\n",
    "        return np.dot(inputs, weights)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(weights):\n",
    "        return weights.T\n",
    "\n",
    "class add_gate:\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(input1, input2):\n",
    "        return input1 + input2\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(input1, input2):\n",
    "        return input1.backward() + input2.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation function\n",
    "\n",
    "In our network, sigmoid function is used. A **sigmoid function** maps any value to a value between 0 and 1.\n",
    "\n",
    "forward\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "backward\n",
    "$$ \\frac{\\partial \\sigma(x)}{\\partial x} =  \\sigma(x)(1- \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid_activation():\n",
    "        \n",
    "    @staticmethod\n",
    "    def forward(net):\n",
    "        return 1/(1 + np.exp(-net))\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(output):\n",
    "        return output*(1 - output)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network layers\n",
    "\n",
    "According to the network architecture shown in first picture, there are three layers in our RNN:\n",
    "    1. input layer\n",
    "    2. hidden layer\n",
    "    3. output layer\n",
    "\n",
    "## Initilize weights\n",
    "\n",
    "By random method in numpy, random weights for each layer is generated.\n",
    "\n",
    "## Forward propagation\n",
    "\n",
    "In every layer, two propagations should be done, forward along with backward. To implement forward, a corresponding method should be implemented in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class random_generator:\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_random_weight_matrix(input_dimension, output_dimension):\n",
    "        return 2*np.random.random((input_dimension,output_dimension)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network_layer(ABC):\n",
    "    \n",
    "    def __init__(self, input_dimension, output_dimension):        \n",
    "        self.weights = random_generator.get_random_weight_matrix(input_dimension, output_dimension)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, input):\n",
    "        pass   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input layer\n",
    "\n",
    "In this layer, inputs including a(t) and b(t) as a vector named x is going to be multiplied by input layer weights in forward phase. \n",
    "\n",
    "**forward**\n",
    "\n",
    "$ net_{input} = x \\times  W_{input} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class input_layer(network_layer):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return multiply_gate.forward(x, self.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden layer\n",
    "\n",
    "### forward\n",
    "\n",
    "In this layer following equations should be implemented in forward propagation:\n",
    "\n",
    "$ net_{hidden} = net_{input} + prev_{hidden} \\times W_{hidden} $\n",
    "\n",
    "sigmoid is used for activation function in this layer\n",
    "\n",
    "$ a_{hidden} = \\sigma(net_{hidden}) $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hidden_layer(network_layer):\n",
    "    \n",
    "    def forward(self, net_input, s_t_prev):        \n",
    "        net_hidden = add_gate.forward(net_input, multiply_gate.forward(s_t_prev, self.weights))\n",
    "        return sigmoid_activation.forward(net_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output layer\n",
    "\n",
    "In this layer following equations should be implemented in forward propagation:\n",
    "\n",
    "\n",
    "### forward\n",
    "\n",
    "$ net_{output} = a_{hidden} \\times  W_{output} $\n",
    "\n",
    "$ \\hat{y}\\ (predited\\ value) = a_{output} = a(net_{output}) = \\sigma(net_{output}) $\n",
    "\n",
    "predited_value = one bit used for the output of the RNN (a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class output_layer(network_layer):\n",
    "    \n",
    "    def forward(self, activation_hidden): \n",
    "        net_output = multiply_gate.forward(activation_hidden, self.weights)    \n",
    "        return sigmoid_activation.forward(net_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary addition RNN\n",
    "\n",
    "In the rest of the code, our RNN is going to be demonstrated.\n",
    "\n",
    "## Initialization\n",
    "\n",
    "As it be explained, the RNN is designed to add two binary arrays, therefore dimention of these arrays is important for initializing.\n",
    "\n",
    "Due to adding two bits in each step, the dimension of input layer is 2, the output of this addition is also one bit, therefore output layer dimension should be 1. There is not optimal number for hidden layer dimension, it could get defined by you.\n",
    "\n",
    "Last point is loss function. It is needed to compute errors and backpropagate it through all layers. **Mean squared error** function is used to compute these errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "The general algorithm is\n",
    "\n",
    "   1. First, present the input pattern and propagate it through the network to get the output.\n",
    "    \n",
    "   2. Then compare the predicted output to the expected output and calculate the error.\n",
    "\n",
    "   3. Then calculate the derivates of the error with respect to the network weights\n",
    "    \n",
    "   4. Try to adjust the weights so that the error is minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "\n",
    "The forward method of binary_addition_rnn iteratively updates the states through time and returns the resulting states (hidden values) as well as predicted values. Figure 3 shows binary addition step by step, as can be seen in it, predecting addition of bits starts from the least significant bit (LSB) to the most significant bit (MSB).\n",
    "\n",
    "<img src=\"./images/binary_addition_steps.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "This phase is as same as train phase excluding back propagation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mse_loss_function():\n",
    "    \n",
    "    def forward(target_value, predicted_value):\n",
    "        return np.mean((target_value - predicted_value)**2)\n",
    "    \n",
    "    def backward(target_value, predicted_value):\n",
    "        predicted_value - target_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class binary_addition_rnn:\n",
    "    \n",
    "    def __init__(self, binary_dim, hidden_dimension, learning_rate):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.binary_dim = binary_dim\n",
    "        input_dimension = 2 # two numbers a, b\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "        output_dimension = 1 # result of addition, c = a+b\n",
    "        \n",
    "        self.input_layer = input_layer(input_dimension, hidden_dimension)\n",
    "        self.hidden_layer = hidden_layer(hidden_dimension, hidden_dimension)\n",
    "        self.output_layer = output_layer(hidden_dimension,output_dimension)\n",
    "        \n",
    "         # predicated_values array\n",
    "        self.predicated_values = np.zeros(self.binary_dim)\n",
    "\n",
    "    def feed_forward(self, a, b, c):        \n",
    "        \n",
    "        hidden_values = list()\n",
    "        hidden_values.append(np.zeros((1, self.hidden_dimension)))\n",
    "        \n",
    "        prediction_values = deque([])\n",
    "        \n",
    "        # Proceed from right-to-left, column-by-column, starting from last digit\n",
    "        for column in range(self.binary_dim-1, -1, -1):\n",
    "            \n",
    "            # It is given two input digits at each time step. \n",
    "            X = np.array([[a[column], b[column]]])\n",
    "            \n",
    "            # input layer\n",
    "            net_input_layer = self.input_layer.forward(X) # X*W_in\n",
    "            \n",
    "            # hidden layer\n",
    "            s_t_prev = hidden_values[-1]\n",
    "            activation_hidden = self.hidden_layer.forward(net_input_layer, s_t_prev)\n",
    "            \n",
    "            # save activation_hidden for BPTT\n",
    "            hidden_values.append(activation_hidden)\n",
    "            \n",
    "            # output layer\n",
    "            prediction_value = self.output_layer.forward(activation_hidden)\n",
    "            prediction_values.appendleft(prediction_value)\n",
    "            \n",
    "        return prediction_values, hidden_values\n",
    "\n",
    "    def back_propagate(self, a, b, c, predicated_values, hidden_values):\n",
    "        \n",
    "        # BPTT\n",
    "        dl_dw_output, dl_dw_hidden, dl_dw_input = back_propagation.bptt(a, b, c, predicated_values, hidden_values, hidden_dimension, self.output_layer.weights, self.hidden_layer.weights, self.input_layer.weights, self.binary_dim)\n",
    "        \n",
    "        self.output_layer.weights -= dl_dw_output * self.learning_rate\n",
    "        self.hidden_layer.weights -= dl_dw_hidden * self.learning_rate\n",
    "        self.input_layer.weights -= dl_dw_input * self.learning_rate  \n",
    "\n",
    "    def train(self, dataset_train):\n",
    "           \n",
    "        epochs_count = len(dataset_train)\n",
    "        \n",
    "        # This for loop \"iterates\" multiple times over the training code to optimize our network to the dataset.\n",
    "        for epoch in range(epochs_count):\n",
    "            \n",
    "            overallError = 0            \n",
    "                             \n",
    "            a, b, c = dataset_utility.get_inputs_and_target(dataset_train[epoch])           \n",
    "            \n",
    "            # feed forward propagation\n",
    "            predicated_values, hidden_values = self.feed_forward(a, b, c)\n",
    "            \n",
    "            # back propagation\n",
    "            self.back_propagate(a, b, c, predicated_values, hidden_values)\n",
    "            \n",
    "            # Print out the Progress of the RNN\n",
    "            if (epoch % 1000 == 0):\n",
    "                 utility.print_result(a, b, c, predicated_values, epoch)\n",
    "\n",
    "\n",
    "    def test(self, dataset_test):\n",
    "\n",
    "        epochs_count = len(dataset_test)\n",
    "        \n",
    "        # This for loop \"iterates\" multiple times over the training code to optimize our network to the dataset.\n",
    "        for epoch in range(epochs_count):\n",
    "                        \n",
    "            a, b, c = dataset_utility.get_inputs_and_target(dataset_train[epoch])\n",
    "            \n",
    "            # feed forward propagation\n",
    "            predicated_values, hidden_values = self.feed_forward(a, b, c)\n",
    "            \n",
    "            # Print out the Progress of the RNN\n",
    "            if (epoch % 1000 == 0):\n",
    "                 utility.print_result(a, b, c, predicated_values, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation throw time (BPTT)\n",
    "\n",
    "BPTT works by unrolling all input timesteps. Each timestep has one input time step, one output time step and one copy of the network. Then the errors are calculated and accumulated for each timestep. The network is then rolled back to update the weights.\n",
    "\n",
    "Following weights are used to compute predicted value, therefore, these weights should get updated by BPTT for next iteration. \n",
    "\n",
    "$ W\\_output $\n",
    "\n",
    "$ W\\_hidden $\n",
    "\n",
    "$ W\\_input $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain rule\n",
    "In order to update the weights, chain rule could help us. Using this rule, the following equations are obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "Mean squared error is used for loss function:\n",
    "\n",
    "$ l = \\frac{1}{2}(y - \\hat{y})^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial \\hat{y}} = \\frac{\\partial \\frac{1}{2}(y - \\hat{y})^2 }{\\partial \\hat{y}} = 2 \\times \\frac{1}{2} \\times -1 \\times (y - \\hat{y}) = \\hat{y} - y $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial net_{output}} = \\frac{\\partial l}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial net_{output}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial net_{output}} = \\frac{\\partial l}{\\partial \\hat{y}} \\times \\frac{\\partial \\sigma({net_{output})}}{\\partial net_{output}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial net_{output}} = (\\hat{y} - y) \\times \\sigma(net_{output}) \\bigodot(1-\\sigma(net_{output})) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial net_{output}} = (\\hat{y} - y) \\times \\hat{y} \\bigodot(1-\\hat{y}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial net_{output}} $ is needed in following equations. Therefore, it is assumed that\n",
    "\n",
    "$ \\frac{\\partial l}{\\partial net_{output}} = \\delta_{net_{output}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\delta_{net_{output}} = (\\hat{y} - y) \\times \\hat{y} \\bigodot(1-\\hat{y}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating $ W_{output} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$ \\frac{\\partial l}{\\partial W_{output}} = ? $**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial W_{output}} = \\frac{\\partial l}{\\partial net_{output}} \\times \\frac{\\partial net_{output}}{\\partial W_{output}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial W_{output}} = \n",
    "\\frac{\\partial l}{\\partial net_{output}} \\times \\frac{\\partial (a_{hidden} \\times  W_{output})}{\\partial W_{output}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"test\" class=\"equation\">\n",
    "$$ \\frac{\\partial l}{\\partial W_{output}} = \n",
    "\\delta_{net_{output}} \\times a_{hidden}^T\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hidden layer and input layer, $\\frac{\\partial l}{\\partial a_{hidden}}$ should be computed\n",
    "\n",
    "$ \\frac{\\partial l}{\\partial a_{hidden}} = ? $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial a_{hidden}} =  \\frac{\\partial l}{\\partial net_{output}} \\times \\frac{\\partial net_{output}}{\\partial a_{hidden}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial a_{hidden}} =  \\frac{\\partial l}{\\partial net_{output}} \\times \\frac{\\partial (a_{hidden} \\times  W_{output}) }{\\partial a_{hidden}} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial l}{\\partial a_{hidden}} = \\delta_{net_{output}} \\times W_{output}^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial net_{hidden}} = ? $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial net_{hidden}} = \\frac{\\partial l(t)}{\\partial net_{hidden}(t)} +  \\frac{\\partial l(t+1)}{\\partial net_{hidden}(t)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t)}{\\partial net_{hidden}(t)} =  \n",
    "\\frac{\\partial l(t)}{\\partial a_{hidden}(t)} \\times \n",
    "\\frac{\\partial a_{hidden}(t)}{\\partial net_{hidden}(t)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t)}{\\partial net_{hidden}(t)} =  \n",
    "\\frac{\\partial l(t)}{\\partial a_{hidden}(t)} \\times \n",
    "\\frac{\\partial \\sigma(net_{hidden})(t)}{\\partial net_{hidden(t)}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t)}{\\partial net_{hidden}(t)} =  \n",
    "\\frac{\\partial l(t)}{\\partial a_{hidden}(t)} \\times \n",
    "\\sigma(net_{hidden}(t)) \\bigodot(1-\\sigma(net_{hidden})(t))\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial net_{hidden}} =  \n",
    "\\frac{\\partial l}{\\partial a_{hidden}} \\times \n",
    "a_{hidden} \\bigodot(1-a_{hidden})\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t)}{\\partial net_{hidden}(t)} =  \n",
    "(\\delta_{net_{output}(t)} \\times W_{output}^T) \\times \n",
    "(a_{hidden}(t) \\bigodot(1-a_{hidden}(t)))\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t)}{\\partial net_{hidden}(t)} = \\delta_{net_{hidden\\_explicit}(t)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\delta_{net_{hidden\\_explicit}}(t) =  \n",
    "(\\delta_{net_{output}}(t) \\times W_{output}^T) \\times \n",
    "(a_{hidden}(t) \\bigodot(1-a_{hidden}(t)))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t+1)}{\\partial net_{hidden(t)}} = ? $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t+1)}{\\partial net_{hidden}(t)} =  \n",
    "\\frac{\\partial l(t+1)}{\\partial net_{hidden}(t+1)} \\times \n",
    "\\frac{\\partial net_{hidden}(t+1)}{\\partial net_{hidden}(t)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t+1)}{\\partial net_{hidden}(t)} =  \n",
    "\\frac{\\partial l(t+1)}{\\partial net_{hidden}(t+1)} \\times \n",
    "\\frac{\\partial (net_{input}(t+1) + a_{hidden}(t) \\times W_{hidden}) }{\\partial net_{hidden}(t)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t+1)}{\\partial net_{hidden}(t)} =  \n",
    "\\frac{\\partial l(t+1)}{\\partial net_{hidden}(t+1)} \\times \n",
    "\\frac{\\partial (a_{hidden}(t) \\times W_{hidden}) }{\\partial net_{hidden}(t)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t+1)}{\\partial net_{hidden}(t)} =  \n",
    "\\frac{\\partial l(t+1)}{\\partial net_{hidden}(t+1)} \\times \n",
    "\\frac{\\partial (a_{hidden}(t) \\times W_{hidden}) }{\\partial a_{hidden}(t)} \\times \n",
    "\\frac{\\partial a_{hidden}(t) }{\\partial net_{hidden}(t)} \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t+1)}{\\partial net_{hidden}(t)} =  \n",
    "\\delta_{net_{hidden\\_explicit}}(t+1) \\times \n",
    " W_{hidden}^T \\times \n",
    "(a_{hidden}(t) \\bigodot(1-a_{hidden}(t)))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t+1)}{\\partial net_{hidden}(t)} = \\delta_{net_{hidden\\_implixit}(t)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\delta_{net_{hidden\\_implixit}(t)} =   \n",
    "\\delta_{net_{hidden\\_explicit}}(t+1) \\times \n",
    " W_{hidden}^T \\times \n",
    "(a_{hidden}(t) \\bigodot(1-a_{hidden}(t)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the two equations computed above, we have:\n",
    "\n",
    "$ \\frac{\\partial l}{\\partial net_{hidden}} = \\frac{\\partial l(t)}{\\partial net_{hidden}(t)} +  \\frac{\\partial l(t+1)}{\\partial net_{hidden}(t)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\delta_{net_{hidden}(t)} = \\delta_{net_{hidden\\_implixit}(t)} +  \\delta_{net_{hidden\\_explixit}(t)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\delta_{net_{hidden}(t)} = \n",
    "(\\delta_{net_{output}}(t) \\times W_{output}^T) \\times \n",
    "(a_{hidden}(t) \\bigodot(1-a_{hidden}(t))) \n",
    "+\n",
    "\\delta_{net_{hidden\\_explicit}}(t+1) \\times \n",
    " W_{hidden}^T \\times \n",
    "(a_{hidden}(t) \\bigodot(1-a_{hidden}(t)))\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\delta_{net_{hidden}(t)} = \n",
    "(\\delta_{net_{output}}(t) \\times W_{output}^T + \\delta_{net_{hidden\\_explicit}}(t+1) \\times \n",
    " W_{hidden}^T)\n",
    "\\times \n",
    "(a_{hidden}(t) \\bigodot(1-a_{hidden}(t))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating $ W_{hidden} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial w_{hidden}} = ? $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t)}{\\partial w_{hidden}} = \n",
    "\\frac{\\partial l(t)}{\\partial net_{hidden}(t)}\n",
    "\\times\n",
    "\\frac{\\partial net_{hidden}(t)}{\\partial w_{hidden}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t)}{\\partial w_{hidden}} = \n",
    "\\delta_{net_{hidden}(t)}\n",
    "\\times\n",
    "\\frac{\\partial net_{hidden}(t)}{\\partial w_{hidden}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial net_{hidden}(t)}{\\partial w_{hidden}} =\n",
    "\\frac{\\partial \\ (net_{input}(t) + prev_{hidden} \\times W_{hidden})}{\\partial w_{hidden}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial net_{hidden}(t)}{\\partial w_{hidden}} =\n",
    "\\frac{\\partial \\ prev_{hidden} \\times W_{hidden}}{\\partial w_{hidden}} = prev_{hidden}^T\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"test\" class=\"equation\">\n",
    "    \n",
    "$$ \\frac{\\partial l(t)}{\\partial w_{hidden}} = \n",
    "\\delta_{net_{hidden}(t)}\n",
    "\\times\n",
    "prev_{hidden}^T\n",
    "$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating $ W_{input} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial w_{input}} = ? $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t)}{\\partial w_{input}} = \n",
    "\\frac{\\partial l(t)}{\\partial net_{hidden}(t)}\n",
    "\\times\n",
    "\\frac{\\partial net_{hidden}(t)}{\\partial w_{input}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t)}{\\partial w_{hidden}} = \n",
    "\\delta_{net_{hidden}(t)}\n",
    "\\times\n",
    "\\frac{\\partial net_{hidden}(t)}{\\partial w_{input}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial net_{hidden}(t)}{\\partial w_{hidden}} =\n",
    "\\frac{\\partial \\ (net_{input}(t) + prev_{hidden} \\times W_{hidden})}{\\partial w_{input}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial net_{hidden}(t)}{\\partial w_{hidden}} =\n",
    "\\frac{\\partial net_{input}(t)}{\\partial w_{input}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial net_{hidden}(t)}{\\partial w_{hidden}} = \n",
    "\\frac{\\partial (x(t) \\times  W_{input})}{\\partial w_{input}} = x(t)^T\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"test\" class=\"equation\">\n",
    "\n",
    "$$ \\frac{\\partial l(t)}{\\partial w_{input}} = \n",
    "\\delta_{net_{hidden}(t)}\n",
    "\\times\n",
    "x(t)^T\n",
    "$$\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class back_propagation:\n",
    "    \n",
    "    @staticmethod\n",
    "    def bptt(a, b, c, predicated_values, hidden_values, hidden_dimension, output_layer_weights, hidden_layer_weights, input_layer_weights, binary_dim):  \n",
    "              \n",
    "        future_delta_net_hidden_explicit = np.zeros(hidden_dimension)\n",
    "        \n",
    "        # Initialize Updated Weights Values\n",
    "        dl_dw_output = np.zeros_like(output_layer_weights)\n",
    "        dl_dw_hidden = np.zeros_like(hidden_layer_weights)\n",
    "        dl_dw_input = np.zeros_like(input_layer_weights)\n",
    "        \n",
    "        for time_step in range(binary_dim):\n",
    "            \n",
    "            # s_t = h(t)\n",
    "            # s_t_prev = h(t-1)\n",
    "            time_step_hidden_value_index = binary_dim - time_step\n",
    "            s_t = hidden_values[time_step_hidden_value_index]\n",
    "            s_t_prev = hidden_values[time_step_hidden_value_index -1]            \n",
    "            \n",
    "            X = np.array([[a[time_step],b[time_step]]])            \n",
    "            y = np.array([[c[time_step]]]).T\n",
    "            y_hat = predicated_values[time_step]           \n",
    "           \n",
    "            # delta_net_output\n",
    "            dl_d_y_hat = y_hat-y\n",
    "            dy_hat_d_net_output = y_hat*(1-y_hat)\n",
    "            delta_net_output = dl_d_y_hat * dy_hat_d_net_output          \n",
    "            \n",
    "            # delta_net_hidden(t)           \n",
    "            delta_net_hidden = (delta_net_output.dot(output_layer_weights.T) + future_delta_net_hidden_explicit.dot(hidden_layer_weights.T))* sigmoid_activation.backward(s_t)\n",
    "            \n",
    "            # save delta_net_hidden_explicit as future_delta_net_hidden_explicit for next backpropagation step\n",
    "            future_delta_net_hidden_explicit = delta_net_output.dot(output_layer_weights.T) * sigmoid_activation.backward(s_t)\n",
    "                        \n",
    "            # Updating W_output, W_hidden and  W_output for every bit\n",
    "            dl_dw_output += np.atleast_2d(s_t).T.dot(delta_net_output)\n",
    "            dl_dw_hidden += np.atleast_2d(s_t_prev).T.dot(delta_net_hidden) \n",
    "            dl_dw_input  += X.T.dot(delta_net_hidden)            \n",
    "        \n",
    "        return dl_dw_output, dl_dw_hidden, dl_dw_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ckecking our network\n",
    "\n",
    "In following code, the network is going to be checked for computing binary addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- train --------------\n",
      "epoch: 0\n",
      "a:   [0 0 1 0 1 1 0 0]\n",
      "b:   [0 0 1 0 1 1 1 1]\n",
      "----------------------\n",
      "c:   [0 1 0 1 1 0 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "============================\n",
      "epoch: 1000\n",
      "a:   [0 1 0 1 0 0 0 1]\n",
      "b:   [0 0 0 1 1 1 0 1]\n",
      "----------------------\n",
      "c:   [0 1 1 0 1 1 1 0]\n",
      "Pred:[0 1 1 1 1 1 1 1]\n",
      "============================\n",
      "epoch: 2000\n",
      "a:   [0 0 1 1 1 0 1 1]\n",
      "b:   [0 0 1 0 1 0 1 1]\n",
      "----------------------\n",
      "c:   [0 1 1 0 0 1 1 0]\n",
      "Pred:[0 1 1 1 0 1 1 1]\n",
      "============================\n",
      "epoch: 3000\n",
      "a:   [0 0 1 0 0 0 0 1]\n",
      "b:   [0 1 0 1 0 1 0 0]\n",
      "----------------------\n",
      "c:   [0 1 1 1 0 1 0 1]\n",
      "Pred:[1 0 1 1 0 1 0 1]\n",
      "============================\n",
      "epoch: 4000\n",
      "a:   [0 1 1 1 0 1 0 0]\n",
      "b:   [0 0 1 1 1 1 0 1]\n",
      "----------------------\n",
      "c:   [1 0 1 1 0 0 0 1]\n",
      "Pred:[1 0 1 1 0 0 0 1]\n",
      "============================\n",
      "epoch: 5000\n",
      "a:   [0 1 1 0 1 0 1 1]\n",
      "b:   [0 0 0 0 1 0 0 1]\n",
      "----------------------\n",
      "c:   [0 1 1 1 0 1 0 0]\n",
      "Pred:[0 1 1 1 0 1 0 0]\n",
      "============================\n",
      "epoch: 6000\n",
      "a:   [0 0 0 0 0 0 0 0]\n",
      "b:   [0 1 1 1 0 0 1 1]\n",
      "----------------------\n",
      "c:   [0 1 1 1 0 0 1 1]\n",
      "Pred:[0 1 1 1 0 0 1 1]\n",
      "============================\n",
      "-------------- test --------------\n",
      "epoch: 0\n",
      "a:   [0 0 0 1 1 0 0 1]\n",
      "b:   [0 1 0 0 0 1 0 0]\n",
      "----------------------\n",
      "c:   [0 1 0 1 1 1 0 1]\n",
      "Pred:[0 1 0 1 1 1 0 1]\n",
      "============================\n",
      "epoch: 1000\n",
      "a:   [0 1 0 0 1 1 1 1]\n",
      "b:   [0 0 0 1 1 0 1 0]\n",
      "----------------------\n",
      "c:   [0 1 1 0 1 0 0 1]\n",
      "Pred:[0 1 1 0 1 0 0 1]\n",
      "============================\n",
      "epoch: 2000\n",
      "a:   [0 1 0 0 1 0 1 1]\n",
      "b:   [0 0 1 0 1 0 1 0]\n",
      "----------------------\n",
      "c:   [0 1 1 1 0 1 0 1]\n",
      "Pred:[0 1 1 1 0 1 0 1]\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "binary_dim = 8\n",
    "hidden_dimension = 16\n",
    "learning_rate = 0.1\n",
    "    \n",
    "# datasets config\n",
    "dataset_size = 10000\n",
    "train_dataset_size = 7000    \n",
    "data = dataset_utility.get_data(dataset_size, binary_dim)\n",
    "dataset_train = data[:train_dataset_size]\n",
    "dataset_test = data[train_dataset_size:]\n",
    "    \n",
    "# initializing RNN\n",
    "rnn = binary_addition_rnn(binary_dim, hidden_dimension, learning_rate)\n",
    "    \n",
    "# train\n",
    "print(\"-------------- train --------------\")\n",
    "rnn.train(dataset_train)\n",
    "    \n",
    "# test\n",
    "print(\"-------------- test --------------\")    \n",
    "rnn.train(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is added to CSS stles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".equation {\n",
       "border: 1px solid;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
