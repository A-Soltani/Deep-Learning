{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary addition\n",
    "_What exactly will the RNN learn?_\n",
    "\n",
    "**RNN is going to learn the carry bit on its own!**\n",
    "\n",
    "\n",
    "| input1 | input2 | carry-in | sum | carry-out |\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "| 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 1 | 1 | 0 |\n",
    "| 0 | 1 | 0 | 1 | 0 |\n",
    "| 0 | 1 | 1 | 0 | 1 |\n",
    "| 1 | 0 | 0 | 1 | 0 |\n",
    "| 1 | 0 | 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 | 0 | 1 |\n",
    "| 1 | 1 | 1 | 1 | 1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# importing \"collections\" for deque operations \n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples\n",
    "To train and test our RNN, data set is needed.\n",
    "Samples in the dataset include a, b and c. Following samples show how they are shown:\n",
    "\n",
    "| a | b | c | \n",
    "| :---: | :---: | :---:|\n",
    "| [0 0 0 1 0 0 1 0] | [0 1 0 0 1 1 0 1] | [0 1 0 1 1 1 1 1]\n",
    "| [0 0 0 0 1 1 1 0] | [0 0 0 1 1 0 0 1] | [0 0 1 0 0 1 1 1]\n",
    "| [0 1 0 1 0 0 1 1] | [0 0 1 0 1 1 1 1] | [1 0 0 0 0 0 1 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good practice to seed your random numbers. Your numbers will still be randomly distributed, but they'll be randomly distributed in exactly the same way each time you train. This makes it easier to see how your changes affect the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_utility:\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_samples(samples_count, binary_dim):\n",
    "        \n",
    "        largest_number = pow(2,binary_dim)\n",
    "        \n",
    "        samples = list()\n",
    "        for i in range(samples_count):\n",
    "            \n",
    "            a = np.random.randint(largest_number/2) \n",
    "            b = np.random.randint(largest_number/2)\n",
    "\n",
    "            # true answer => summation\n",
    "            c = a + b            \n",
    "            \n",
    "            int_sample = np.array([[a], [b], [c]], dtype=np.uint8)            \n",
    "            binary_int_sample = np.unpackbits(int_sample, axis=1)\n",
    "            \n",
    "            samples.append(binary_array)\n",
    "            \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture\n",
    "Following architecture is used to addition of two bits in each step:\n",
    "\n",
    "<img src=\"./images/network_architecture.jpg\"><img>\n",
    "<center>Figure 1</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation model\n",
    "\n",
    "Following model is used to compute addition of two bits in each step:\n",
    "\n",
    "<img src=\"./images/forward_one_step.jpg\"><img>\n",
    "<center>Figure 2</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN gates\n",
    "\n",
    "As can be seen in comutation model, there are two types of gates in our RNN:\n",
    "    1. multiply gate\n",
    "    2. add gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiply_gate:\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(inputs, weights):\n",
    "        return np.dot(inputs, weights)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(inputs):\n",
    "        return inputs.T\n",
    "\n",
    "class add_gate:\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(input1, input2):\n",
    "        return input1 + input2\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(input1, input2):\n",
    "        return input1.backward() + input2.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation function\n",
    "\n",
    "In our network, sigmoid function is used. A **sigmoid function** maps any value to a value between 0 and 1.\n",
    "\n",
    "forward\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "backward\n",
    "$$ \\frac{\\partial \\sigma(x)}{\\partial x} =  \\sigma(x)(1- \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid_activation():\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    def backward(self, x):\n",
    "        return x*(1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network layers\n",
    "\n",
    "According to the network architecture shown in first picture, there are three layers in our RNN:\n",
    "    1. input layer\n",
    "    2. hidden layer\n",
    "    3. output layer\n",
    "\n",
    "## Initilize weights\n",
    "\n",
    "By random method in numpy, random weights for each layer is generated.\n",
    "\n",
    "## Forward propagation\n",
    "\n",
    "In every layer, two propagations should be done, forward along with backward. To implement forward, a corresponding method should be implemented in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class random_generator:\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_random_weight_matrix(input_dimension, output_dimension):\n",
    "        return 2*np.random.random((input_dimension,output_dimension)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network_layer(ABC):\n",
    "    \n",
    "    def __init__(self, input_dimension, output_dimension):        \n",
    "        self.weights = random_generator.get_random_weight_matrix(input_dimension, output_dimension)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, input):\n",
    "        pass   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input layer\n",
    "\n",
    "In this layer, inputs including a(t) and b(t) as a vector named x is going to be multiplied by input layer weights in forward phase. \n",
    "\n",
    "**forward**\n",
    "\n",
    "$ net_{input} = x \\times  W_{input} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class input_layer(network_layer):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return multiply_gate.forward(x, self.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden layer\n",
    "\n",
    "### forward\n",
    "\n",
    "In this layer following equations should be implemented in forward propagation:\n",
    "\n",
    "$ net_{hidden} = net_{input} + prev_{hidden} \\times W_{hidden} $\n",
    "\n",
    "sigmoid is used for activation function in this layer\n",
    "\n",
    "$ a_{hidden} = \\sigma(net_{hidden}) $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hidden_layer(network_layer):\n",
    "    \n",
    "    def forward(self, net_input, s_t_prev):        \n",
    "        net_hidden = add_gate.forward(net_input, multiply_gate.forward(s_t_prev, self.weights))\n",
    "        return sigmoid_activation.forward(net_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output layer\n",
    "\n",
    "In this layer following equations should be implemented in forward propagation:\n",
    "\n",
    "\n",
    "### forward\n",
    "\n",
    "$ net_{output} = a_{hidden} \\times  W_{output} $\n",
    "\n",
    "$ \\hat{y}\\ (predited\\ value) = a_{output} = a(net_{output}) = \\sigma(net_{output}) $\n",
    "\n",
    "predited_value = one bit used for the output of the RNN (a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class output_layer(network_layer):\n",
    "    \n",
    "    def forward(self, activation_hidden): \n",
    "        net_output = multiply_gate.forward(activation_hidden, self.weights)    \n",
    "        return sigmoid_activation.forward(net_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary addition RNN\n",
    "\n",
    "In the rest of the code, our RNN is going to be demonstrated.\n",
    "\n",
    "## Initialization\n",
    "\n",
    "As it be explained, the RNN is designed to add two binary arrays, therefore dimention of these arrays is important for initializing.\n",
    "\n",
    "Due to adding two bits in each step, the dimension of input layer is 2, the output of this addition is also one bit, therefore output layer dimension should be 1. There is not optimal number for hidden layer dimension, it could get defined by you.\n",
    "\n",
    "Last point is loss function. It is needed to compute errors and backpropagate it through all layers. **Mean squared error** function is used to compute these errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mse_loss_function():\n",
    "    \n",
    "    def forward(target_value, predicted_value):\n",
    "        return np.mean((target_value - predicted_value)**2)\n",
    "    \n",
    "    def backward(target_value, predicted_value):\n",
    "        predicted_value - target_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class binary_addition_rnn:\n",
    "    \n",
    "    def __init__(self, binary_dim, hidden_dimension):\n",
    "        \n",
    "        self.binary_dim = binary_dim\n",
    "        input_layer_dimension = 2 # two numbers a, b\n",
    "        self.hidden_layer_dimension = hidden_dimension\n",
    "        output_dimension = 1 # result of addition, c = a + b\n",
    "        \n",
    "        self.input_layer = input_layer(input_layer_dimension, hidden_dimension)\n",
    "        self.hidden_layer = hidden_layer(hidden_dimension, hidden_dimension)\n",
    "        self.output_layer = output_layer(hidden_dimension,output_dimension)\n",
    "        \n",
    "        self.loss_function = mse_loss_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "\n",
    "The forward method of binary_addition_rnn iteratively updates the states through time and returns the resulting states (hidden values) as well as predicted values. Figure 3 shows binary addition step by step, as can be seen in it, predecting addition of bits starts from the least significant bit (LSB) to the most significant bit (MSB).\n",
    "\n",
    "<img src=\"./images/binary_addition_steps.gif\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def feed_forward(self, a, b, c):        \n",
    "        \n",
    "        hidden_values = list()\n",
    "        hidden_values.append(np.zeros((1, self.hidden_dimension)))\n",
    "        \n",
    "        prediction_values = deque([])\n",
    "        \n",
    "        # Proceed from right-to-left, column-by-column, starting from last digit\n",
    "        for column in range(self.binary_dim-1, -1, -1):\n",
    "            \n",
    "            # It is given two input digits at each time step. \n",
    "            X = np.array([[a[column], b[column]]])\n",
    "            \n",
    "            # input layer\n",
    "            net_input_layer = self.input_layer.forward(X) # X*W_in\n",
    "            \n",
    "            # hidden layer\n",
    "            s_t_prev = hidden_values[-1]\n",
    "            activation_hidden = self.hidden_layer.forward(net_input_layer, s_t_prev)            \n",
    "            # save activation_hidden for BPTT\n",
    "            hidden_values.append(activation_hidden)\n",
    "            \n",
    "            # output layer\n",
    "            prediction_value = self.output_layer.forward(activation_hidden)\n",
    "            # save predicted values for BPTT\n",
    "            prediction_values.appendleft(prediction_value)\n",
    "            \n",
    "        return prediction_values, hidden_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation throw time (BPTT)\n",
    "\n",
    "BPTT works by unrolling all input timesteps. Each timestep has one input time step, one output time step and one copy of the network. Then the errors are calculated and accumulated for each timestep. The network is then rolled back to update the weights.\n",
    "\n",
    "Following weights are used to compute predicted value, therefore, these weights should get updated by BPTT for next iteration. \n",
    "\n",
    "$ W\\_output $\n",
    "\n",
    "$ W\\_hidden $\n",
    "\n",
    "$ W\\_input $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain rule\n",
    "In order to update the weights, chain rule could help us. Using this rule, the following equations are obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean squared error is used for loss function:\n",
    "\n",
    "$ l = \\frac{1}{2}(y - \\hat{y})^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial \\hat{y}} = \\frac{\\partial \\frac{1}{2}(y - \\hat{y})^2 }{\\partial \\hat{y}} = 2 \\times \\frac{1}{2} \\times -1 \\times (y - \\hat{y}) = \\hat{y} - y $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial net_{output}} = \\frac{\\partial l}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial net_{output}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial net_{output}} = \\frac{\\partial l}{\\partial \\hat{y}} \\times \\frac{\\partial \\sigma({net_{output})}}{\\partial net_{output}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial net_{output}} = (\\hat{y} - y) \\times \\sigma(net_{output}) \\bigodot(1-\\sigma(net_{output})) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial net_{output}} = (\\hat{y} - y) \\times \\hat{y} \\bigodot(1-\\hat{y}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial net_{output}} $ is needed in following equations. Therefore, it is assumed that\n",
    "\n",
    "$ \\frac{\\partial l}{\\partial net_{output}} = \\delta_{net_{output}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\delta_{net_{output}} = (\\hat{y} - y) \\times \\hat{y} \\bigodot(1-\\hat{y}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Updating $ W_{output} $**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$ \\frac{\\partial l}{\\partial W_{output}} = ? $**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial W_{output}} = \\frac{\\partial l}{\\partial net_{output}} \\times \\frac{\\partial net_{output}}{\\partial W_{output}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial W_{output}} = \n",
    "\\frac{\\partial l}{\\partial net_{output}} \\times \\frac{\\partial (a_{hidden} \\times  W_{output})}{\\partial W_{output}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"test\" class=\"equation\">\n",
    "$$ \\frac{\\partial l}{\\partial W_{output}} = \n",
    "\\delta_{net_{output}} \\times a_{hidden}^T\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hidden layer and input layer, $\\frac{\\partial l}{\\partial a_{hidden}}$ should be computed\n",
    "\n",
    "$ \\frac{\\partial l}{\\partial a_{hidden}} = ? $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial a_{hidden}} =  \\frac{\\partial l}{\\partial net_{output}} \\times \\frac{\\partial net_{output}}{\\partial a_{hidden}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial a_{hidden}} =  \\frac{\\partial l}{\\partial net_{output}} \\times \\frac{\\partial (a_{hidden} \\times  W_{output}) }{\\partial a_{hidden}} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial l}{\\partial a_{hidden}} = \\delta_{net_{output}} \\times W_{output}^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial net_{hidden}} = ? $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial net_{hidden}} = \\frac{\\partial l(t)}{\\partial net_{hidden}(t)} +  \\frac{\\partial l(t+1)}{\\partial net_{hidden}(t)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t)}{\\partial net_{hidden}(t)} =  \n",
    "\\frac{\\partial l(t)}{\\partial a_{hidden}(t)} \\times \n",
    "\\frac{\\partial a_{hidden}(t)}{\\partial net_{hidden}(t)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t)}{\\partial net_{hidden}(t)} =  \n",
    "\\frac{\\partial l(t)}{\\partial a_{hidden}(t)} \\times \n",
    "\\frac{\\partial \\sigma(net_{hidden})(t)}{\\partial net_{hidden(t)}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t)}{\\partial net_{hidden}(t)} =  \n",
    "\\frac{\\partial l(t)}{\\partial a_{hidden}(t)} \\times \n",
    "\\sigma(net_{hidden}(t)) \\bigodot(1-\\sigma(net_{hidden})(t))\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial net_{hidden}} =  \n",
    "\\frac{\\partial l}{\\partial a_{hidden}} \\times \n",
    "a_{hidden} \\bigodot(1-a_{hidden})\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t)}{\\partial net_{hidden}(t)} =  \n",
    "(\\delta_{net_{output}(t)} \\times W_{output}^T) \\times \n",
    "(a_{hidden}(t) \\bigodot(1-a_{hidden}(t)))\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t)}{\\partial net_{hidden}(t)} = \\delta_{net_{hidden\\_explicit}(t)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\delta_{net_{hidden\\_explicit}}(t) =  \n",
    "(\\delta_{net_{output}}(t) \\times W_{output}^T) \\times \n",
    "(a_{hidden}(t) \\bigodot(1-a_{hidden}(t)))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t+1)}{\\partial net_{hidden(t)}} = ? $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t+1)}{\\partial net_{hidden}(t)} =  \n",
    "\\frac{\\partial l(t+1)}{\\partial net_{hidden}(t+1)} \\times \n",
    "\\frac{\\partial net_{hidden}(t+1)}{\\partial net_{hidden}(t)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t+1)}{\\partial net_{hidden}(t)} =  \n",
    "\\frac{\\partial l(t+1)}{\\partial net_{hidden}(t+1)} \\times \n",
    "\\frac{\\partial (net_{input}(t+1) + a_{hidden}(t) \\times W_{hidden}) }{\\partial net_{hidden}(t)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t+1)}{\\partial net_{hidden}(t)} =  \n",
    "\\frac{\\partial l(t+1)}{\\partial net_{hidden}(t+1)} \\times \n",
    "\\frac{\\partial (a_{hidden}(t) \\times W_{hidden}) }{\\partial net_{hidden}(t)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t+1)}{\\partial net_{hidden}(t)} =  \n",
    "\\frac{\\partial l(t+1)}{\\partial net_{hidden}(t+1)} \\times \n",
    "\\frac{\\partial (a_{hidden}(t) \\times W_{hidden}) }{\\partial a_{hidden}(t)} \\times \n",
    "\\frac{\\partial a_{hidden}(t) }{\\partial net_{hidden}(t)} \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t+1)}{\\partial net_{hidden}(t)} =  \n",
    "\\delta_{net_{hidden\\_explicit}}(t+1) \\times \n",
    " W_{hidden}^T \\times \n",
    "(a_{hidden}(t) \\bigodot(1-a_{hidden}(t)))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t+1)}{\\partial net_{hidden}(t)} = \\delta_{net_{hidden\\_implixit}(t)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\delta_{net_{hidden\\_implixit}(t)} =   \n",
    "\\delta_{net_{hidden\\_explicit}}(t+1) \\times \n",
    " W_{hidden}^T \\times \n",
    "(a_{hidden}(t) \\bigodot(1-a_{hidden}(t)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the two equations computed above, we have:\n",
    "\n",
    "$ \\frac{\\partial l}{\\partial net_{hidden}} = \\frac{\\partial l(t)}{\\partial net_{hidden}(t)} +  \\frac{\\partial l(t+1)}{\\partial net_{hidden}(t)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\delta_{net_{hidden}(t)} = \\delta_{net_{hidden\\_implixit}(t)} +  \\delta_{net_{hidden\\_explixit}(t)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\delta_{net_{hidden}(t)} = \n",
    "(\\delta_{net_{output}}(t) \\times W_{output}^T) \\times \n",
    "(a_{hidden}(t) \\bigodot(1-a_{hidden}(t))) \n",
    "+\n",
    "\\delta_{net_{hidden\\_explicit}}(t+1) \\times \n",
    " W_{hidden}^T \\times \n",
    "(a_{hidden}(t) \\bigodot(1-a_{hidden}(t)))\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\delta_{net_{hidden}(t)} = \n",
    "(\\delta_{net_{output}}(t) \\times W_{output}^T + \\delta_{net_{hidden\\_explicit}}(t+1) \\times \n",
    " W_{hidden}^T)\n",
    "\\times \n",
    "(a_{hidden}(t) \\bigodot(1-a_{hidden}(t))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$ W_{hidden} $**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial w_{hidden}} = ? $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t)}{\\partial w_{hidden}} = \n",
    "\\frac{\\partial l(t)}{\\partial net_{hidden}(t)}\n",
    "\\times\n",
    "\\frac{\\partial net_{hidden}(t)}{\\partial w_{hidden}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t)}{\\partial w_{hidden}} = \n",
    "\\delta_{net_{hidden}(t)}\n",
    "\\times\n",
    "\\frac{\\partial net_{hidden}(t)}{\\partial w_{hidden}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial net_{hidden}(t)}{\\partial w_{hidden}} =\n",
    "\\frac{\\partial \\ (net_{input}(t) + prev_{hidden} \\times W_{hidden})}{\\partial w_{hidden}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial net_{hidden}(t)}{\\partial w_{hidden}} =\n",
    "\\frac{\\partial \\ prev_{hidden} \\times W_{hidden}}{\\partial w_{hidden}} = prev_{hidden}^T\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"test\" class=\"equation\">\n",
    "    \n",
    "$$ \\frac{\\partial l(t)}{\\partial w_{hidden}} = \n",
    "\\delta_{net_{hidden}(t)}\n",
    "\\times\n",
    "prev_{hidden}^T\n",
    "$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$ W_{input} $**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l}{\\partial w_{input}} = ? $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t)}{\\partial w_{input}} = \n",
    "\\frac{\\partial l(t)}{\\partial net_{hidden}(t)}\n",
    "\\times\n",
    "\\frac{\\partial net_{hidden}(t)}{\\partial w_{input}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial l(t)}{\\partial w_{hidden}} = \n",
    "\\delta_{net_{hidden}(t)}\n",
    "\\times\n",
    "\\frac{\\partial net_{hidden}(t)}{\\partial w_{input}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial net_{hidden}(t)}{\\partial w_{hidden}} =\n",
    "\\frac{\\partial \\ (net_{input}(t) + prev_{hidden} \\times W_{hidden})}{\\partial w_{input}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial net_{hidden}(t)}{\\partial w_{hidden}} =\n",
    "\\frac{\\partial net_{input}(t)}{\\partial w_{input}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial net_{hidden}(t)}{\\partial w_{hidden}} = \n",
    "\\frac{\\partial (x(t) \\times  W_{input})}{\\partial w_{input}} = x(t)^T\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"test\" class=\"equation\">\n",
    "\n",
    "$$ \\frac{\\partial l(t)}{\\partial w_{input}} = \n",
    "\\delta_{net_{hidden}(t)}\n",
    "\\times\n",
    "x(t)^T\n",
    "$$\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   def bptt(self, a, b, c, predicated_values, hidden_values):\n",
    "        \n",
    "        future_hidden_delta = np.zeros(self.hidden_dimension)\n",
    "        future_hidden = np.zeros(self.hidden_dimension)\n",
    "        \n",
    "        # Initialize Updated Weights Values\n",
    "        W_output_update = np.zeros_like(self.output_layer.weights)\n",
    "        W_hidden_update = np.zeros_like(self.hidden_layer.weights)\n",
    "        W_input_update = np.zeros_like(self.input_layer.weights)\n",
    "        \n",
    "        for time_step in range(self.binary_dim):\n",
    "            \n",
    "            # s_t = h(t)\n",
    "            time_step_hidden_value_index = self.binary_dim - time_step\n",
    "            s_t = hidden_values[time_step_hidden_value_index]\n",
    "            s_t_prev = hidden_values[time_step_hidden_value_index -1]\n",
    "            \n",
    "            # target value\n",
    "            y = np.array([[c[time_step]]]).T\n",
    "            X = np.array([[a[time_step],b[time_step]]])\n",
    "            \n",
    "            y_hat = predicated_values[time_step]  \n",
    "            \n",
    "            # loss = y-y_hat\n",
    "            # delta\n",
    "           \n",
    "            \n",
    "            delta_y_hat = self.loss_function.backward(y, y_hat)\n",
    "            \n",
    "            \n",
    "            # hidden_delta = delta_3 + future_hidden_delta.dot(self.hidden_layer.weights.T) * (future_hidden*(1-future_hidden))\n",
    "            hidden_delta = future_hidden_delta.dot(self.hidden_layer.weights.T) * sigmoid_activation.backward(future_hidden) + delta_2\n",
    "\n",
    "            # W_output\n",
    "            \n",
    "            \n",
    "            # W_input\n",
    "            x = np.array([[a[time_step], b[time_step]]])\n",
    "            x_prev = np.array([[a[time_step-1], b[time_step-1]]])\n",
    "            \n",
    "            \n",
    "\n",
    "            # error at output layer\n",
    "            outputlayer_error = y - y_hat\n",
    "            outputlayer_delta = (outputlayer_error)*sigmoid_activation.backward(y_hat)*(-1)\n",
    "        \n",
    "            # error at hidden layer * sigmoid_derivative(future_hidden)\n",
    "            hidden_delta = (future_hidden_delta.dot(self.hidden_layer.weights.T) * sigmoid_activation.backward(future_hidden) + outputlayer_delta.dot(self.output_layer.weights.T)) * sigmoid_activation.backward(s_t)\n",
    "\n",
    "            \n",
    "            # update all weights \n",
    "            W_output_update += np.atleast_2d(s_t).T.dot(outputlayer_delta)\n",
    "            W_hidden_update += np.atleast_2d(s_t_prev).T.dot(hidden_delta) \n",
    "            # W_input_update  += X.T.dot(hidden_delta) \n",
    "            W_input_update  += self.input_layer.backward.dot(hidden_delta) \n",
    "            future_hidden_delta = hidden_delta\n",
    "            future_hidden = s_t \n",
    "        \n",
    "        return W_output_update, W_hidden_update, W_input_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hiddenLayerUnfold:\n",
    "    \n",
    "    def __init__(self, neuron_count):\n",
    "        \n",
    "        # Save the values obtained at Hidden Layer of current state in a list to keep track\n",
    "        self.hidden_layer_values  = list()\n",
    "        \n",
    "        # Initially, there is no previous hidden state. So append \"0\" for that\n",
    "        self.hidden_layer_values.append(np.zeros(neuron_count))\n",
    "    \n",
    "    def save_previous_hidden_layer_value(self, previous_hidden_layer_value):\n",
    "        self.hidden_layer_values.append(copy.deepcopy(previous_hidden_layer_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'network_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b58ff2ee0df9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mhidden_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneuron_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneuron_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m#self.hiddenLayerUnfold = hiddenLayerUnfold(neuron_count)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'network_layer' is not defined"
     ]
    }
   ],
   "source": [
    "class hidden_layer(network_layer):\n",
    "    \n",
    "    def __init__(self, neuron_count):\n",
    "        super().__init__(neuron_count)\n",
    "        #self.hiddenLayerUnfold = hiddenLayerUnfold(neuron_count)\n",
    "        # Save the values obtained at Hidden Layer of current state in a list to keep track\n",
    "        self.hidden_layer_values  = list()\n",
    "        \n",
    "        # Initially, there is no previous hidden state. So append \"0\" for that\n",
    "        self.hidden_layer_values.append(np.zeros(neuron_count))\n",
    "    \n",
    "    def forward(self, input_layer_output, W_hidden):\n",
    "        prev_hidden = self.hidden_layer_values[-1]      \n",
    "        net_hidden = input_layer_output + np.dot(prev_hidden, W_hidden)\n",
    "        sigmoid = sigmoid_activation()\n",
    "        return sigmoid.forward(net_hidden)\n",
    "    \n",
    "    def backward(self, hidden_value_index, W_hidden, binary_dim):\n",
    "        if hidden_value_index == -binary_dim:\n",
    "            s_0 = self.hidden_layer_values[0]\n",
    "            return s_0\n",
    "        \n",
    "        s_t = self.hidden_layer_values[hidden_value_index]\n",
    "        t1 = s_t*(1-s_t)\n",
    "        s_t_1 = self.hidden_layer_values[hidden_value_index-1]\n",
    "        \n",
    "        backward_prev = self.backward(hidden_value_index-1, W_hidden, binary_dim)\n",
    "        t2 = backward_prev * W_hidden\n",
    "        t3 = s_t_1 + t2\n",
    "        return_value =  t1 * t3\n",
    "        \n",
    "        return return_value\n",
    "    \n",
    "    def save_previous_hidden_layer_value(self, previous_hidden_layer_value):\n",
    "        #self.hiddenLayerUnfold.save_previous_hidden_layer_value(previous_hidden_layer_value)\n",
    "        self.hidden_layer_values.append(copy.deepcopy(previous_hidden_layer_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class output_layer(network_layer):\n",
    "    \n",
    "    def forward(self, hidden_layer_output, W_output):\n",
    "        net_output = np.dot(hidden_layer_output, W_output)\n",
    "        sigmoid = sigmoid_activation()\n",
    "        return sigmoid.forward(net_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weight:\n",
    "    \n",
    "    @staticmethod\n",
    "    def GetWeightMatrix(first_dimension, second_dimension):\n",
    "        return 2*np.random.random((first_dimension,second_dimension)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_function():\n",
    "    \n",
    "    @staticmethod\n",
    "    def mse(target_value, predicted_value):\n",
    "        return np.mean((target_value - predicted_value)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class utility:\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_result(overallError, a_int, b_int, c, d):    \n",
    "        print(\"Error:\" + str(overallError))\n",
    "        print(\"Pred:\" + str(d))\n",
    "        print(\"True:\" + str(c))\n",
    "        out = 0\n",
    "        for index, x in enumerate(reversed(d)):\n",
    "            out += x * pow(2, index)\n",
    "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "The general algorithm is\n",
    "\n",
    "   1. First, present the input pattern and propagate it through the network to get the output.\n",
    "    \n",
    "   2. Then compare the predicted output to the expected output and calculate the error.\n",
    "\n",
    "   3. Then calculate the derivates of the error with respect to the network weights\n",
    "    \n",
    "   4. Try to adjust the weights so that the error is minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "\n",
    "This is our process to predict summation of two bits in each step:\n",
    "\n",
    "**input layer**\n",
    "\n",
    "X = two input bits (a,b)\n",
    "\n",
    "$ net_{input} = X \\times  W_{input} $\n",
    "\n",
    "No activation function is used in this layer\n",
    "\n",
    "**hidden layer**\n",
    "\n",
    "$ net_{hidden} = net_{input} + prev_{hidden} \\times W_{hidden} $\n",
    "\n",
    "activation function, which is used in this layer, is sigmoid\n",
    "\n",
    "$ A_{hidden} = A(net_{hidden}) = \\sigma(net_{hidden}) $\n",
    "\n",
    "**output layer**\n",
    "\n",
    "$ net_{output} = A_{hidden} \\times  W_{output} $\n",
    "\n",
    "$ \\hat{y}\\ (predited\\ value) = A_{output} = A(net_{output}) = \\sigma(net_{output}) $\n",
    "\n",
    "predited_value = one bit used for the output of the RNN (a+b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that unlike the corresponding equation in the previous example, the\n",
    "equations for $ W_{hidden}$ and $W_{input}$ are **recursive**.\n",
    "\n",
    "The derivative at the current time depends on the derivative at the previous time.\n",
    "\n",
    "**$ W_{hidden} $**\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial W_{hidden}} = \\frac{\\partial E}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial net_{output}} \\times \\frac{\\partial net_{output}}{\\partial A_{hidden}} \\times \\frac{\\partial A_{hidden}}{\\partial net_{hidden}} \\times \\frac{\\partial net_{hidden}}{\\partial W_{hidden}} $$\n",
    "\n",
    "To compute equation above, following computation are needed:\n",
    "\n",
    "$ \\frac{\\partial E}{\\partial \\hat{y}} = -(y - \\hat{y}) $\n",
    "\n",
    "$ \\frac{\\partial \\hat{y}}{\\partial net_{output}} = \\hat{y}(1-\\hat{y})$\n",
    "\n",
    "$ \\frac{\\partial net_{output}}{\\partial A_{hidden}} =  \\frac{\\partial A_{hidden} \\times  W_{output}}{\\partial  A_{hidden}} = w_{output}$\n",
    "\n",
    "$ \\frac{\\partial A_{hidden}}{\\partial net_{hidden}} = \\frac{\\partial \\sigma (net_{hidden})} {\\partial net_{hidden}} =  \\sigma (net_{hidden}) (1-\\sigma (net_{hidden})) = A_{hidden}(1-A_{hidden})$\n",
    "\n",
    "$ \\frac{\\partial net_{hidden}}{\\partial w_{hidden}} =  \\frac{\\partial \\ (net_{input} + prev_{hidden} \\times W_{hidden})}{\\partial w_{hidden}} = prev_{hidden} + \\frac{\\partial prev_{hidden}}{\\partial w_{hidden}}\\times w_{hidden}$\n",
    "\n",
    "By putting these results into the main equation, we end up with following equation:\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial W_{hidden}} = -(y - \\hat{y}) \\times \\hat{y}(1-\\hat{y}) \\times w_{output} \\times A_{hidden}(1-A_{hidden}) \\times (prev_{hidden} + \\frac{\\partial prev_{hidden}}{\\partial w_{hidden}}\\times w_{hidden})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$ W_{input} $**\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial W_{input}} = \\frac{\\partial E}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial net_{output}} \\times \\frac{\\partial net_{output}}{\\partial A_{hidden}} \\times \\frac{\\partial A_{hidden}}{\\partial net_{hidden}} \\times \\frac{\\partial net_{hidden}}{\\partial net_{input}} \\times \\frac{\\partial net_{input}}{\\partial W_{input}}$$\n",
    "\n",
    "To compute equation above, following computation are needed:\n",
    "\n",
    "$ \\frac{\\partial E}{\\partial \\hat{y}} = -(y - \\hat{y}) $\n",
    "\n",
    "$ \\frac{\\partial \\hat{y}}{\\partial net_{output}} = \\hat{y}(1-\\hat{y})$\n",
    "\n",
    "$ \\frac{\\partial net_{output}}{\\partial A_{hidden}} = w_{output}$\n",
    "\n",
    "$ \\frac{\\partial A_{hidden}}{\\partial W_{input}} = \\frac{\\partial A_{hidden}}{\\partial net_{hidden}} \\times \\frac{\\partial net_{hidden}}{\\partial W_{input}} $\n",
    "\n",
    "$ \\frac{\\partial A_{hidden}}{\\partial net_{hidden}} = A_{hidden}(1-A_{hidden}) $\n",
    "\n",
    "$ \\frac{\\partial net_{hidden}}{\\partial W_{input}} =  \\frac{\\partial \\ (net_{input} + prev_{hidden} \\times W_{hidden})}{\\partial W_{input}} =  \\frac{\\partial \\ (X \\times  W_{input} + prev_{hidden} \\times W_{hidden})}{\\partial W_{input}} = X +  \\frac{\\partial \\ ( prev_{hidden} \\times W_{hidden})}{\\partial W_{input}} = X + \\frac{\\partial \\ prev_{hidden}}{\\partial W_{input} }\\times W_{hidden} $\n",
    "\n",
    "$ \\frac{\\partial A_{hidden}}{\\partial W_{input}} = A_{hidden}(1-A_{hidden}) \\times (X + \\frac{\\partial \\ prev_{hidden}}{\\partial W_{input} }\\times W_{hidden}) $\n",
    "\n",
    "By putting these results into main equation, we end up with following equation:\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial W_{input}} = -(y - \\hat{y}) \\times \\hat{y}(1-\\hat{y}) \\times w_{output} \\times A_{hidden}(1-A_{hidden}) \\times (X + \\frac{\\partial prev_{hidden}}{\\partial W_{input} }\\times W_{hidden}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_binary_addition_rnn:\n",
    "    \n",
    "    def __init__(self, binary_dim, hidden_dimension, learning_rate):\n",
    "        \n",
    "        self.binary_dim = binary_dim\n",
    "        input_dimension = 2\n",
    "        output_dimension = 1    \n",
    "        \n",
    "        # predicated_values\n",
    "        self.predicated_values = np.zeros(self.binary_dim)        \n",
    "        \n",
    "        # layers\n",
    "        self.input_layer = input_layer(input_dimension)\n",
    "        self.hidden_layer = hidden_layer(hidden_dimension)\n",
    "        self.output_layer = output_layer(output_dimension)\n",
    "        \n",
    "        # initialize weights\n",
    "        self.W_input = weight.GetWeightMatrix(input_dimension, hidden_dimension)\n",
    "        self.W_hidden = weight.GetWeightMatrix(hidden_dimension, hidden_dimension)\n",
    "        self.W_output = weight.GetWeightMatrix(hidden_dimension, output_dimension)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.overallError = 0\n",
    "        \n",
    "    def feed_forward(self, a, b, c):\n",
    "        \n",
    "         # Array to save predicted outputs (binary encoded)\n",
    "        d = np.zeros_like(c)\n",
    "    \n",
    "        # position: location of the bit amongst binary_dim-1 bits; for example, starting point \"0\"; \"0 - 7\"\n",
    "        for position in range(binary_dim):\n",
    "\n",
    "            location = binary_dim - position - 1\n",
    "            X = np.array([[a[location], b[location]]])           \n",
    "            \n",
    "            # ----------- forward ---------------\n",
    "            # input_layer forward\n",
    "            input_layer_output = self.input_layer.forward(X, self.W_input)            \n",
    "            \n",
    "            # hidden_layer forward\n",
    "            hidden_layer_output = self.hidden_layer.forward(input_layer_output, self.W_hidden)\n",
    "            \n",
    "            # Save the hidden layer to be used in BPTT            \n",
    "            self.hidden_layer.save_previous_hidden_layer_value(hidden_layer_output)\n",
    "            \n",
    "            # self.output_layer.forward\n",
    "            # predicated_value is a \"guess\" for each input matrix. \n",
    "            # We can now compare how well it did by subtracting the true answer (y) from the guess (predicated_value). \n",
    "            # output_error is just a vector of positive and negative numbers reflecting how much the network missed.\n",
    "            predicated_value = self.output_layer.forward(hidden_layer_output, self.W_output)          \n",
    "\n",
    "            # Round off the values to nearest \"0\" or \"1\" and save it to a list\n",
    "            d[location] = np.round(predicated_value[0][0])   \n",
    "            \n",
    "        return d, self.predicated_values\n",
    "\n",
    "    def back_propagate(self, a, b, c, predicated_values):\n",
    "        \n",
    "        # Initialize Updated Weights Values\n",
    "        W_output_update = np.zeros_like(self.W_output)\n",
    "        W_hidden_update = np.zeros_like(self.W_hidden)\n",
    "        W_input_update = np.zeros_like(self.W_input)        \n",
    "        \n",
    "        # for position in range(self.binary_dim-1, -1, -1):  # binary_dim=8=> position: 7->0\n",
    "        for position in range(self.binary_dim):           \n",
    "\n",
    "            y = np.array([[c[position]]]).T        \n",
    "            \n",
    "            # sigmoid\n",
    "            sigmoid = sigmoid_activation()\n",
    "          \n",
    "            hidden_value_index = -position-1\n",
    "            A_hidden = self.hidden_layer.hidden_layer_values[hidden_value_index]\n",
    "          \n",
    "            # update W_output ----------------------------------------------------         \n",
    "            y_hat = predicated_values[position]            \n",
    "            dy_hat = (y-y_hat)\n",
    "            \n",
    "            # W_output---------------------\n",
    "            dnet_output = dy_hat * sigmoid.backward(y_hat)\n",
    "            dw_output = dnet_output* A_hidden.T           \n",
    "            W_output_update += dw_output     \n",
    "\n",
    "            # W_hidden ---------------------\n",
    "            dA_hidden = dnet_output*self.W_output            \n",
    "                    \n",
    "            t3 = self.hidden_layer.backward(hidden_value_index, self.W_hidden, self.binary_dim)\n",
    "            t4 = dA_hidden*t3            \n",
    "            W_hidden_update += t4        \n",
    "            \n",
    "            # W_input ---------------------\n",
    "            t_in_3 = self.input_layer.backward(a, b, hidden_value_index, self.W_hidden, self.binary_dim, self.hidden_layer.hidden_layer_values)\n",
    "            t_in_4 = dA_hidden*t_in_3            \n",
    "            W_input_update += t_in_4\n",
    "            \n",
    "            \n",
    "        self.W_output += W_output_update * self.learning_rate\n",
    "        self.W_hidden += W_hidden_update * self.learning_rate\n",
    "        self.W_input += W_input_update * self.learning_rate\n",
    "   \n",
    "    \n",
    "    def train(self, epochs_count):\n",
    "        \n",
    "        data = dataset(self.binary_dim)        \n",
    "        \n",
    "        # This for loop \"iterates\" multiple times over the training code to optimize our network to the dataset.\n",
    "        for epoch in range(epochs_count):\n",
    "            \n",
    "            overallError = 0\n",
    "            \n",
    "            # sample a + b = c\n",
    "            # for example: 2 + 3 = 5 => (a) 00000010 + (b) 00000011 = (c) 00000101\n",
    "            a, b, c, a_int, b_int, c_int = data.get_sample_addition_problem()\n",
    "            \n",
    "            # where we'll store our best guess (binary encoded)\n",
    "            # desired predictions => d\n",
    "            d = np.zeros_like(c)  \n",
    "            \n",
    "            d, predicated_values = self.feed_forward(a, b, c)\n",
    "            \n",
    "            self.back_propagate(a, b, c, predicated_values)\n",
    "    \n",
    "            # Print out the Progress of the RNN\n",
    "            if (epoch % 1000 == 0):\n",
    "                utility.print_result(overallError, a_int, b_int, c, d)\n",
    "                \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dim = 8\n",
    "hidden_dimension = 16\n",
    "learning_rate = 0.1\n",
    "rnn = simple_binary_addition_rnn(binary_dim, hidden_dimension, learning_rate)\n",
    "rnn.train(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".equation {\n",
       "border: 1px solid;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
