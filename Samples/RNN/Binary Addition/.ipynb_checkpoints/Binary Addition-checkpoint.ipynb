{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Jupyter notebooks do not have intellisense. If you like to enable it, add following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable intellisense\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary addition\n",
    "_What exactly will the RNN learn ?_\n",
    "\n",
    "**RNN is going to learn the carry bit on its own!**\n",
    "\n",
    "\n",
    "| input1 | input2 | carry-in | sum | carry-out |\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "| 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 1 | 1 | 0 |\n",
    "| 0 | 1 | 0 | 1 | 0 |\n",
    "| 0 | 1 | 1 | 0 | 1 |\n",
    "| 1 | 0 | 0 | 1 | 0 |\n",
    "| 1 | 0 | 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 | 0 | 1 |\n",
    "| 1 | 1 | 1 | 1 | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples\n",
    "The first step, sample data is needed.\n",
    "One looup table is used to help us converting int to binary and vice versa\n",
    "\n",
    "int2binary (__lookup table__)\n",
    "\n",
    "| int | binary array |\n",
    "| :--- | :---: |\n",
    "| 0 | [0, 0, 0, 0, 0, 0, 0, 0] |\n",
    "| 1 | [0, 0, 0, 0, 0, 0, 0, 1] |\n",
    "| 2 | [0, 0, 0, 0, 0, 0, 1, 0] |\n",
    "...\n",
    "| 255 | [1, 1, 1, 1, 1, 1, 1, 1] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, numpy as np\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    def __init__(self, binary_dim):\n",
    "        # creating lookup table for converting int to binary\n",
    "        self.int2binary = {}\n",
    "        \n",
    "        self.largest_number = pow(2,binary_dim)\n",
    "        range_numbers = range(self.largest_number)\n",
    "        \n",
    "        # genrating corresponding binary array\n",
    "        # for example binary[0] = array([0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)\n",
    "        binary = np.unpackbits(np.array([range_numbers],dtype=np.uint8).T,axis=1)\n",
    "        \n",
    "        # adding binary array to int2binary (lookup table)\n",
    "        for i in range_numbers:\n",
    "            self.int2binary[i] = binary[i]\n",
    "    \n",
    "    # generate a sample addition problem (a + b = c)\n",
    "    @staticmethod\n",
    "    def get_sample_addition_problem(self):\n",
    "        a_int = np.random.randint(self.largest_number/2) # int version # generate random int between [1,largest_number/2)\n",
    "        a = self.int2binary[a_int] # binary encoding\n",
    "\n",
    "        b_int = np.random.randint(self.largest_number/2) # int version\n",
    "        b = self.int2binary[b_int] # binary encoding\n",
    "\n",
    "        # true answer => summation\n",
    "        c_int = a_int + b_int\n",
    "        c = self.int2binary[c_int]\n",
    "\n",
    "        return a, b, c, a_int, b_int, c_int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activation(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(net):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(output):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sigmoid activation function**\n",
    "\n",
    "forward\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "backward\n",
    "$$ \\frac{\\partial \\sigma(x)}{\\partial x} =  \\sigma(x)(1- \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid_activation(activation):\n",
    "        \n",
    "    def forward(net):\n",
    "        return 1/(1 + np.exp(-net))\n",
    "    \n",
    "    def backward(output):\n",
    "        return output*(1 - output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In every layer, number of neurons along with activation function should be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network_layer(ABC):\n",
    "    \n",
    "    def __init__(self, neuron_count):\n",
    "        self.neuron_count = neuron_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class input_layer(network_layer): \n",
    "    \n",
    "    def forward(X, W_input):\n",
    "        return np.dot(X,W_input)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hidden_layer(network_layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Save the values obtained at Hidden Layer of current state in a list to keep track\n",
    "        self.hidden_layer_values  = list()\n",
    "        \n",
    "        # Initially, there is no previous hidden state. So append \"0\" for that\n",
    "        self.hidden_layer_values.append(np.zeros(self.neuron_count))\n",
    "        \n",
    "    def save_previous_hidden_layer_value(previous_hidden_layer_value):\n",
    "        self.hidden_layer_values.append(copy.deepcopy(previous_hidden_layer_value))\n",
    "    \n",
    "    def forward(self, input, W_hidden):\n",
    "        prev_hidden = self.hidden_layer_values[-1]\n",
    "        net_hidden = input + np.dot(prev_hidden, W_hidden)\n",
    "        return self.sigmoid_activation.forward(net_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class output_layer(network_layer):\n",
    "    \n",
    "    def forward(hidden_layer_output, W_output):\n",
    "        net_output = np.dot(hidden_layer_output, W_output)\n",
    "        return self.sigmoid_activation.forward(net_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weight:\n",
    "    \n",
    "    @staticmethod\n",
    "    def GetWeightMatrix(first_dimension, second_dimension):\n",
    "        return 2*np.random.random((first_dimension,second_dimension)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_function(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def compute(target_value, predicted_value):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean squared error function**\n",
    "\n",
    "https://en.wikipedia.org/wiki/Mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_function(loss_function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def mse(target_value, predicted_value):\n",
    "        return np.mean((target_value - predicted_value)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class utility:\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_result(overallError, a_int, b_int, c, d):    \n",
    "        print(\"Error:\" + str(overallError))\n",
    "        print(\"Pred:\" + str(d))\n",
    "        print(\"True:\" + str(c))\n",
    "        out = 0\n",
    "        for index, x in enumerate(reversed(d)):\n",
    "            out += x * pow(2, index)\n",
    "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-271c5511a9c6>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-271c5511a9c6>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    self.input_layer = new input_layer(input_dimension)\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class simple_binary_addition_rnn:\n",
    "    \n",
    "    def __init__(self,binary_dim, input_dimension, output_dimension, hidden_dimension, learning_rate):\n",
    "        \n",
    "        self.binary_dim = binary_dim\n",
    "        \n",
    "        # layers\n",
    "        self.input_layer = new input_layer(input_dimension)\n",
    "        self.hidden_layer = new hidden_layer(hidden_dimension)\n",
    "        self.output_layer = new output_layer(output_dimension)\n",
    "        \n",
    "        # initialize weights\n",
    "        self.W_input = weight.GetWeightMatrix(input_dimension, hidden_dimension)\n",
    "        self.W_hidden = weight.GetWeightMatrix(hidden_dimension, hidden_dimension)\n",
    "        self.W_output = weight.GetWeightMatrix(hidden_dimension, output_dimension)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.overallError = 0\n",
    "        \n",
    "        # update values for weights\n",
    "        self.output_layer_deltas = list()\n",
    "        \n",
    "    def feed_forward(a, b, c):   \n",
    "    \n",
    "        # position: location of the bit amongst binary_dim-1 bits; for example, starting point \"0\"; \"0 - 7\"\n",
    "        for position in range(binary_dim):\n",
    "\n",
    "            location = binary_dim - position - 1\n",
    "            X = np.array([[a[location], b[location]]])\n",
    "\n",
    "            # Actual value for (a+b) = c, c is an array of 8 bits, so take transpose to compare bit by bit with X value.        \n",
    "            target = np.array([[c[location]]]).T            \n",
    "            \n",
    "            # ----------- forward ---------------\n",
    "            # input_layer forward\n",
    "            input_layer_output = self.input_layer.forward(X,W_in)\n",
    "            \n",
    "            # hidden_layer forward\n",
    "            hidden_layer_output = self.hidden_layer.forward(input_layer_output, self.W_hidden)\n",
    "            #net_1 = np.dot(X,W_in) + np.dot(hidden_layer_values[-1],W_h) \n",
    "            \n",
    "            # self.output_layer.forward\n",
    "            predicated_value = self.output_layer.forward(hidden_layer_output, W_output)            \n",
    "        \n",
    "            # Save the hidden layer to be used later            \n",
    "            #hidden_layer_values.append(copy.deepcopy(layer_1_output))             \n",
    "            # ToDo\n",
    "            self.hidden_layer.save_previous_hidden_layer_value(predicated_value)\n",
    "            \n",
    "            # Delta rule\n",
    "            # Calculate the error\n",
    "            # Cost function\n",
    "            # C = 1/2 x (y_true - y_pred)^2\n",
    "            output_error = target - layer_2_output\n",
    "\n",
    "            # Save the error deltas at each step as it will be propagated back\n",
    "            output_layer_deltas.append((output_error)*sigmoid_output_to_derivative(layer_2_output))\n",
    "\n",
    "            # Save the sum of error at each binary position\n",
    "            overallError += np.abs(output_error[0])\n",
    "\n",
    "            # Round off the values to nearest \"0\" or \"1\" and save it to a list\n",
    "            d[location] = np.round(layer_2_output[0][0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __train__(self, epochs_count):\n",
    "        \n",
    "        for epoch in range(epochs_count):\n",
    "            \n",
    "            # sample a + b = c\n",
    "            # for example: 2 + 3 = 5 => (a) 00000010 + (b) 00000011 = (c) 00000101\n",
    "            a, b, c, a_int, b_int, c_int = dataset.get_sample_addition_problem()\n",
    "            \n",
    "            # where we'll store our best guess (binary encoded)\n",
    "            # desired predictions => d\n",
    "            d = np.zeros_like(c)  \n",
    "            \n",
    "            feed_forward(a, b ,c)\n",
    "            \n",
    "            back_propagating(a, b)\n",
    "    \n",
    "            # Print out the Progress of the RNN\n",
    "            if (epoch % 1000 == 0):\n",
    "                print_result(overallError, a_int, b_int, c, d)\n",
    "                \n",
    "    \n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
