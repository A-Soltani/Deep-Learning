{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Jupyter notebooks do not have intellisense. If you like to enable it, add following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable intellisense\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary addition\n",
    "_What exactly will the RNN learn ?_\n",
    "\n",
    "**RNN is going to learn the carry bit on its own!**\n",
    "\n",
    "\n",
    "| input1 | input2 | carry-in | sum | carry-out |\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "| 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 1 | 1 | 0 |\n",
    "| 0 | 1 | 0 | 1 | 0 |\n",
    "| 0 | 1 | 1 | 0 | 1 |\n",
    "| 1 | 0 | 0 | 1 | 0 |\n",
    "| 1 | 0 | 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 | 0 | 1 |\n",
    "| 1 | 1 | 1 | 1 | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples\n",
    "The first step, sample data is needed.\n",
    "One looup table is used to help us converting int to binary and vice versa\n",
    "\n",
    "int2binary (__lookup table__)\n",
    "\n",
    "| int | binary array |\n",
    "| :--- | :---: |\n",
    "| 0 | [0, 0, 0, 0, 0, 0, 0, 0] |\n",
    "| 1 | [0, 0, 0, 0, 0, 0, 0, 1] |\n",
    "| 2 | [0, 0, 0, 0, 0, 0, 1, 0] |\n",
    "...\n",
    "| 255 | [1, 1, 1, 1, 1, 1, 1, 1] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, numpy as np\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good practice to seed your random numbers. Your numbers will still be randomly distributed, but they'll be randomly distributed in exactly the same way each time you train. This makes it easier to see how your changes affect the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset1:\n",
    "    def __init__(self, binary_dim):\n",
    "        # creating lookup table for converting int to binary\n",
    "        self.int2binary = {}\n",
    "        \n",
    "        self.largest_number = pow(2,binary_dim)\n",
    "        range_numbers = range(self.largest_number)\n",
    "        \n",
    "        # genrating corresponding binary array\n",
    "        # for example binary[0] = array([0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)\n",
    "        binary = np.unpackbits(np.array([range_numbers],dtype=np.uint8).T,axis=1)\n",
    "        \n",
    "        # adding binary array to int2binary (lookup table)\n",
    "        for i in range_numbers:\n",
    "            self.int2binary[i] = binary[i]\n",
    "    \n",
    "    # generate a sample addition problem (a + b = c)\n",
    "    def get_sample_addition_problem(self):\n",
    "        a_int = np.random.randint(self.largest_number/2) # int version # generate random int between [1,largest_number/2)\n",
    "        a = self.int2binary[a_int] # binary encoding\n",
    "\n",
    "        b_int = np.random.randint(self.largest_number/2) # int version\n",
    "        b = self.int2binary[b_int] # binary encoding\n",
    "\n",
    "        # true answer => summation\n",
    "        c_int = a_int + b_int\n",
    "        c = self.int2binary[c_int]\n",
    "\n",
    "        return a, b, c, a_int, b_int, c_int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activation(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(net):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(output):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sigmoid activation function**\n",
    "\n",
    "A sigmoid function maps any value to a value between 0 and 1. \n",
    "\n",
    "forward\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "backward\n",
    "$$ \\frac{\\partial \\sigma(x)}{\\partial x} =  \\sigma(x)(1- \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid_activation(activation):\n",
    "        \n",
    "    def forward(net):\n",
    "        return 1/(1 + np.exp(-net))\n",
    "    \n",
    "    def backward(output):\n",
    "        return output*(1 - output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In every layer, number of neurons along with activation function should be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network_layer(ABC):\n",
    "    \n",
    "    def __init__(self, neuron_count):\n",
    "        self.neuron_count = neuron_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class input_layer(network_layer): \n",
    "    \n",
    "    def forward(X, W_input):\n",
    "        return np.dot(X,W_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-267-ba03b9d8b92a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0minp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "binary_dim = 8\n",
    "hidden_dimension = 16\n",
    "learning_rate = 0.1\n",
    "rnn = simple_binary_addition_rnn(binary_dim, hidden_dimension, learning_rate)\n",
    "\n",
    "rnn.W_input\n",
    "data = dataset1(binary_dim)       \n",
    "      \n",
    "a, b, c, a_int, b_int, c_int = data.get_sample_addition_problem()\n",
    "location = binary_dim - 0 - 1\n",
    "X = np.array([[a[location], b[location]]])\n",
    "X\n",
    "inp = input_layer(2)\n",
    "inp.forward(X, rnn.W_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hiddenLayerUnfold:\n",
    "    \n",
    "    def __init__(self, neuron_count):\n",
    "        \n",
    "        # Save the values obtained at Hidden Layer of current state in a list to keep track\n",
    "        self.hidden_layer_values  = list()\n",
    "        \n",
    "        # Initially, there is no previous hidden state. So append \"0\" for that\n",
    "        self.hidden_layer_values.append(np.zeros(neuron_count))\n",
    "    \n",
    "    def save_previous_hidden_layer_value(previous_hidden_layer_value):\n",
    "        self.hidden_layer_values.append(copy.deepcopy(previous_hidden_layer_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hidden_layer(network_layer):\n",
    "    \n",
    "    def __init__(self, neuron_count):\n",
    "        super().__init__(neuron_count)\n",
    "        self.hiddenLayerUnfold = hiddenLayerUnfold(neuron_count)\n",
    "    \n",
    "    def forward(self, input, W_hidden):\n",
    "        prev_hidden = self.hidden_layer_values[-1]\n",
    "        net_hidden = input + np.dot(prev_hidden, W_hidden)\n",
    "        return self.sigmoid_activation.forward(net_hidden)\n",
    "    \n",
    "    def save_previous_hidden_layer_value(previous_hidden_layer_value):\n",
    "        self.hiddenLayerUnfold.save_previous_hidden_layer_value(previous_hidden_layer_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class output_layer(network_layer):\n",
    "    \n",
    "    def forward(hidden_layer_output, W_output):\n",
    "        net_output = np.dot(hidden_layer_output, W_output)\n",
    "        return self.sigmoid_activation.forward(net_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weight:\n",
    "    \n",
    "    @staticmethod\n",
    "    def GetWeightMatrix(first_dimension, second_dimension):\n",
    "        return 2*np.random.random((first_dimension,second_dimension)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean squared error function**\n",
    "\n",
    "https://en.wikipedia.org/wiki/Mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_function():\n",
    "    \n",
    "    @staticmethod\n",
    "    def mse(target_value, predicted_value):\n",
    "        return np.mean((target_value - predicted_value)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class utility:\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_result(overallError, a_int, b_int, c, d):    \n",
    "        print(\"Error:\" + str(overallError))\n",
    "        print(\"Pred:\" + str(d))\n",
    "        print(\"True:\" + str(c))\n",
    "        out = 0\n",
    "        for index, x in enumerate(reversed(d)):\n",
    "            out += x * pow(2, index)\n",
    "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "\n",
    "This is our process to predict summation of two bits in each step:\n",
    "\n",
    "**input layer**\n",
    "\n",
    "X = two input bits (a,b)\n",
    "\n",
    "$ net_{input} = X \\times  W_{in} $\n",
    "\n",
    "No activation function is used in this layer\n",
    "\n",
    "**hidden layer**\n",
    "\n",
    "$ net_{hidden} = net_{input} + prev_{hidden} \\times W_{hidden} $\n",
    "\n",
    "activation function, which is used in this layer, is sigmoid\n",
    "\n",
    "$ A_{hidden} = A(net_{hidden}) = \\sigma(net_{hidden}) $\n",
    "\n",
    "**output layer**\n",
    "\n",
    "$ net_{output} = A_{hidden} \\times  W_{output} $\n",
    "\n",
    "$ \\hat{y}\\ (predited\\ value) = A_{output} = A(net_{output}) = \\sigma(net_{output}) $\n",
    "\n",
    "predited_value = one bit used for the output of the RNN (a+b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation throw time (BPTT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_binary_addition_rnn:\n",
    "    \n",
    "    def __init__(self, binary_dim, hidden_dimension, learning_rate):\n",
    "        \n",
    "        self.binary_dim = binary_dim\n",
    "        input_dimension = 2\n",
    "        output_dimension = 1    \n",
    "        \n",
    "        # layers\n",
    "        self.input_layer = input_layer(input_dimension)\n",
    "        self.hidden_layer = hidden_layer(hidden_dimension)\n",
    "        self.output_layer = output_layer(output_dimension)\n",
    "        \n",
    "        # initialize weights\n",
    "        self.W_input = weight.GetWeightMatrix(input_dimension, hidden_dimension)\n",
    "        self.W_hidden = weight.GetWeightMatrix(hidden_dimension, hidden_dimension)\n",
    "        self.W_output = weight.GetWeightMatrix(hidden_dimension, output_dimension)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.overallError = 0\n",
    "        \n",
    "        # update values for weights\n",
    "        self.output_layer_deltas = list()\n",
    "        \n",
    "    def feed_forward(self, a, b, c):   \n",
    "    \n",
    "        # position: location of the bit amongst binary_dim-1 bits; for example, starting point \"0\"; \"0 - 7\"\n",
    "        for position in range(binary_dim):\n",
    "\n",
    "            location = binary_dim - position - 1\n",
    "            X = np.array([[a[location], b[location]]])\n",
    "\n",
    "            # Actual value for (a+b) = c, c is an array of 8 bits, so take transpose to compare bit by bit with X value.        \n",
    "            target = np.array([[c[location]]]).T            \n",
    "            \n",
    "            # ----------- forward ---------------\n",
    "            # input_layer forward\n",
    "            input_layer_output = self.input_layer.forward(X, self.W_input)\n",
    "            \n",
    "            # hidden_layer forward\n",
    "            hidden_layer_output = self.hidden_layer.forward(input_layer_output, self.W_hidden)\n",
    "            #net_1 = np.dot(X,W_in) + np.dot(hidden_layer_values[-1],W_h) \n",
    "            \n",
    "            # self.output_layer.forward\n",
    "            # predicated_value is a \"guess\" for each input matrix. \n",
    "            # We can now compare how well it did by subtracting the true answer (y) from the guess (predicated_value). \n",
    "            # output_error is just a vector of positive and negative numbers reflecting how much the network missed.\n",
    "            predicated_value = self.output_layer.forward(hidden_layer_output, self.W_output)            \n",
    "        \n",
    "            # Save the hidden layer to be used later            \n",
    "            #hidden_layer_values.append(copy.deepcopy(layer_1_output))             \n",
    "            # ToDo\n",
    "            self.hidden_layer.save_previous_hidden_layer_value(predicated_value)          \n",
    "\n",
    "            # Round off the values to nearest \"0\" or \"1\" and save it to a list\n",
    "            d[location] = np.round(predicated_value[0][0])   \n",
    "            \n",
    "        return d\n",
    "    \n",
    "    \n",
    "    def train(self, epochs_count):\n",
    "        \n",
    "        data = dataset1(binary_dim)\n",
    "        \n",
    "        # This for loop \"iterates\" multiple times over the training code to optimize our network to the dataset.\n",
    "        for epoch in range(epochs_count):\n",
    "            \n",
    "            # sample a + b = c\n",
    "            # for example: 2 + 3 = 5 => (a) 00000010 + (b) 00000011 = (c) 00000101\n",
    "            a, b, c, a_int, b_int, c_int = data.get_sample_addition_problem()\n",
    "            \n",
    "            # where we'll store our best guess (binary encoded)\n",
    "            # desired predictions => d\n",
    "            d = np.zeros_like(c)  \n",
    "            \n",
    "            d = self.feed_forward(a, b ,c)\n",
    "            \n",
    "            #back_propagating(a, b)\n",
    "    \n",
    "            # Print out the Progress of the RNN\n",
    "            if (epoch % 10 == 0):\n",
    "                print_result(overallError, a_int, b_int, c, d)\n",
    "                \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dim = 8\n",
    "hidden_dimension = 16\n",
    "learning_rate = 0.1\n",
    "rnn = simple_binary_addition_rnn(binary_dim, hidden_dimension, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset1(binary_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 1, 0, 0, 1], dtype=uint8),\n",
       " array([0, 0, 1, 1, 1, 1, 0, 0], dtype=uint8),\n",
       " array([0, 1, 0, 0, 0, 1, 0, 1], dtype=uint8),\n",
       " 9,\n",
       " 60,\n",
       " 69)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.get_sample_addition_problem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-259-c8a0246ef621>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-255-64cc13926ed7>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs_count)\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;31m#back_propagating(a, b)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-255-64cc13926ed7>\u001b[0m in \u001b[0;36mfeed_forward\u001b[1;34m(self, a, b, c)\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;31m# input_layer forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mw1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0minput_layer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;31m# hidden_layer forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "rnn.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
